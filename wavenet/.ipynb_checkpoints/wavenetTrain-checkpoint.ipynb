{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgPQ8uFiMWwg",
    "outputId": "58e76746-8757-432e-bfb9-a415371b87fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 18:45:11.358534: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-21 18:45:11.358580: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-21 18:45:11.358605: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-21 18:45:11.365954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-21 18:45:12.003225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.fft\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as transformsaudio\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import soundfile as sf\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "directoryBase = \"/home/afridman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q2geUymaNvfH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_max_length(tensor1, tensor2):\n",
    "    max_length = max(tensor1.size(1), tensor2.size(1))\n",
    "\n",
    "    pad_tensor1 = torch.nn.functional.pad(tensor1, (0, max_length - tensor1.size(1)))\n",
    "    pad_tensor2 = torch.nn.functional.pad(tensor2, (0, max_length - tensor2.size(1)))\n",
    "\n",
    "    return pad_tensor1, pad_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KRPe4vgOOROy"
   },
   "outputs": [],
   "source": [
    "inputSize = 32000\n",
    "class AudioCleaningDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, noise_dir, reverb_dir, target_length=inputSize, maxRuido=0.001, fixedInterval=False):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.noise_dir = noise_dir\n",
    "        self.reverb_dir = reverb_dir\n",
    "        self.target_length = target_length\n",
    "        self.resampleo = transformsaudio.Resample(orig_freq=48000, new_freq=16000)  # Resampling\n",
    "        self.resampleoIR = transformsaudio.Resample(orig_freq=32000, new_freq=16000)  # Resampling\n",
    "\n",
    "        self.noise_files = os.listdir(noise_dir) # List all noise files\n",
    "        self.reverb_files = os.listdir(reverb_dir)\n",
    "        self.maxRuido = maxRuido\n",
    "        self.conv= transformsaudio.Convolve(mode=\"same\")\n",
    "        self.fixedInterval = fixedInterval\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_file_name = os.path.join(self.audio_dir, self.dataframe.iloc[idx]['audio_file_name']+\".wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_name)\n",
    "\n",
    "        waveformOriginal = self.resampleo(waveform)\n",
    "\n",
    "        #aca seleccionar\n",
    "        if(self.fixedInterval):\n",
    "            maximoPosible = 0\n",
    "        else:\n",
    "            maximoPosible = len(waveformOriginal[0])-self.target_length\n",
    "        if(maximoPosible>=0):\n",
    "          comienzoAleatorio = (random.randint(0,maximoPosible))\n",
    "          waveformOriginal = waveformOriginal[:,comienzoAleatorio:comienzoAleatorio+self.target_length]\n",
    "\n",
    "\n",
    "        waveformSucia = waveformOriginal.clone()\n",
    "        padding = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "        # Load a random noise file\n",
    "        noise_file_name = random.choice(self.noise_files)\n",
    "        noise_waveform, sample_Rate_ruido = torchaudio.load(os.path.join(self.noise_dir, noise_file_name))\n",
    "        # Repeat the noise waveform until it's at least as long as the audio waveform\n",
    "        while noise_waveform.size(1) < waveformSucia.size(1):\n",
    "            noise_waveform = torch.cat((noise_waveform, noise_waveform), dim=1)\n",
    "\n",
    "        # Trim the noise waveform to match the length of the audio waveform\n",
    "        noise_waveform = noise_waveform[:,:waveformSucia.size(1)]\n",
    "\n",
    "        # Add noise with a random signal-to-noise ratio between 0.01 and 0.1\n",
    "        snr = random.uniform(0.0, self.maxRuido) #random.uniform(0.01, 0.1)\n",
    "\n",
    "        # LE SACO EL RUIDO SUCIO PARA ACELERAR APRENDIZAJE !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        waveformSucia = waveformSucia + noise_waveform * snr\n",
    "        whitenoise = random.uniform(0.0, self.maxRuido)\n",
    "        waveformSucia = waveformSucia + torch.randn_like(waveformOriginal) * whitenoise\n",
    "        return 1*waveformSucia, 1*waveformOriginal\n",
    "\n",
    "        \"\"\"\n",
    "        IR_file_name = random.choice(self.reverb_files)\n",
    "        IR_waveform, sample_Rate_IR = torchaudio.load(os.path.join(self.reverb_dir, IR_file_name))\n",
    "        IR_waveform = self.resampleoIR(IR_waveform)\n",
    "\n",
    "        # Normalize impulse response\n",
    "        normalized_ir = IR_waveform / (IR_waveform.abs().max())\n",
    "\n",
    "        # Perform convolution\n",
    "        padded_signal, padded_filter = pad_to_max_length(waveformSucia, normalized_ir)\n",
    "\n",
    "        # Perform the FFT\n",
    "        fft_signal = torch.fft.fft(padded_signal)\n",
    "        fft_filter = torch.fft.fft(padded_filter)\n",
    "\n",
    "        # Perform the convolution in the frequency domain\n",
    "        fft_result = fft_signal * fft_filter\n",
    "\n",
    "        # Perform the inverse FFT to get the result in the time domain\n",
    "        result = torch.fft.ifft(fft_result)\n",
    "\n",
    "        # The result is complex, take the real part\n",
    "\n",
    "        # LE SACO EL REVERB PARA ACELERAR APRENDIZAJE !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #waveformSucia = result.real\n",
    "\n",
    "        padding = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "       # waveformOriginal = (waveformOriginal - waveformOriginal.mean()) / waveformOriginal.std()\n",
    "       # waveformSucia = (waveformSucia - waveformSucia.mean()) / waveformSucia.std()\n",
    "       # Optional: Normalize convolved audio\n",
    "       # waveformSucia = waveformSucia / (waveformSucia.abs().max())\n",
    "       # waveformOriginal = waveformOriginal / (waveformOriginal.abs().max())\n",
    "\n",
    "        return 1*waveformSucia, 1*waveformOriginal\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rqo9d72ZAvve"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DilatedCausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super(DilatedCausalConv1d, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(channels, channels,\n",
    "                                    kernel_size=2, stride=1,  # Fixed for WaveNet\n",
    "                                    dilation=dilation,\n",
    "                                    padding=dilation,  # Fixed for WaveNet dilation\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # padding=1 for same size(length) between input and output for causal convolution\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=2, stride=1, padding=1,\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        # remove last value for causal convolution\n",
    "        return output[:, :, :-1]\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation):\n",
    "        \"\"\"\n",
    "        Residual block\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :param dilation:\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dilated = DilatedCausalConv1d(res_channels, dilation=dilation)\n",
    "        self.conv_res = torch.nn.Conv1d(res_channels, res_channels, 1)\n",
    "        self.conv_skip = torch.nn.Conv1d(res_channels, skip_channels, 1)\n",
    "\n",
    "        self.gate_tanh = torch.nn.Tanh()\n",
    "        self.gate_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv_skip = self.conv_skip.to(device)\n",
    "            self.conv_res = self.conv_res.to(device)\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = self.dilated(x)\n",
    "\n",
    "        # PixelCNN gate\n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh * gated_sigmoid\n",
    "\n",
    "        # Residual network\n",
    "        output = self.conv_res(gated)\n",
    "        output = output[:, :, 0:inputSize]\n",
    "\n",
    "        input_cut = x#[:, :, -output.size(2):]\n",
    "\n",
    "        output += input_cut\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip\n",
    "\n",
    "\n",
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "\n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            # output is the next input\n",
    "            output, skip = res_block(output, skip_size)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "\n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "\n",
    "class DensNet(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DensNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.tan = torch.nn.Tanh()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv1 = self.conv1.to(device)\n",
    "            self.conv2 = self.conv2.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output1 = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (0.58.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from numba>=0.51.0->librosa) (0.41.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from pooch>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most energetic interval is from 2.00 seconds to 4.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def find_most_energetic_interval(audio_file):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=48000)\n",
    "\n",
    "    \n",
    "    # Calculate energy\n",
    "    energy = np.square(y)\n",
    "\n",
    "    # If audio is longer than 2 seconds, split into 2-second frames\n",
    "    if len(y) > 32000:\n",
    "        frame_size = 2 * sr  # 2 seconds\n",
    "        num_frames = len(y) // frame_size\n",
    "        frames = np.array_split(energy, num_frames)\n",
    "\n",
    "        # Calculate energy for each frame\n",
    "        frame_energies = [np.sum(frame) for frame in frames]\n",
    "\n",
    "        # Find the frame with highest energy\n",
    "        most_energetic_frame_idx = np.argmax(frame_energies)\n",
    "\n",
    "        # Calculate start and end time of the most energetic interval\n",
    "        start_time = most_energetic_frame_idx * 2\n",
    "        end_time = start_time + 2\n",
    "    else:\n",
    "        start_time = 0\n",
    "        end_time = len(y) / sr\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "# Example usage\n",
    "audio_file = '/home/afridman/extra/audiosDivididos/audioPocos/arf_06592_02124244736.wav'\n",
    "start_time, end_time = find_most_energetic_interval(audio_file)\n",
    "print(f\"The most energetic interval is from {start_time:.2f} seconds to {end_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X1LUHnVDXWGz"
   },
   "outputs": [],
   "source": [
    "#loss_fn = nn.L1Loss()\n",
    "#learning_rate = 0.0015\n",
    "#optimizer = torch.optim.AdamW(params=wavenetModel.parameters(), lr=learning_rate)\n",
    "#optimizerPost = torch.optim.AdamW(params=postnetModel.parameters(), lr=learning_rate)\n",
    "\n",
    "mel_transform1 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 2048,\n",
    "                                                n_mels = 120,\n",
    "                                                hop_length = 512).to('cuda')\n",
    "\n",
    "mel_transform2 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 512,\n",
    "                                                n_mels = 80,\n",
    "                                                hop_length = 128).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredicted = torch.tensor([1, 2, 30, 1], dtype=torch.float32)\\ntrue = torch.tensor([2, 8, 9, 0.01], dtype=torch.float32)\\n\\ncriterion = CustomLoss()\\nloss = criterion(predicted, true)\\nprint(loss.item())'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, true):\n",
    "        loss = 0\n",
    "\n",
    "        for p, t in zip(predicted, true):\n",
    "            print(t[0].shape)\n",
    "            print(t[0])\n",
    "            if(abs(t)<0.001):\n",
    "                t = 0.001\n",
    "            loss += abs((p / t) - 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "predicted = torch.tensor([1, 2, 30, 1], dtype=torch.float32)\n",
    "true = torch.tensor([2, 8, 9, 0.01], dtype=torch.float32)\n",
    "\n",
    "criterion = CustomLoss()\n",
    "loss = criterion(predicted, true)\n",
    "print(loss.item())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lOjB-ReSsR-Y"
   },
   "outputs": [],
   "source": [
    "def prepareSpectogram(melspect):\n",
    "    melspect = (melspect - melspect.min() + 0.00000001) #/ (melspect.max()-melspect.min()) + 0.00000001)\n",
    "    melspect=melspect.log10()\n",
    "    return melspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MelspectogramLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelspectogramLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        mel1true_y = (mel_transform1(true_y.to('cuda')))\n",
    "        mel1y_pred = (mel_transform1(y_pred.to('cuda')))\n",
    "        mel2true_y = (mel_transform2(true_y.to('cuda')))\n",
    "        mel2y_pred = (mel_transform2(y_pred.to('cuda')))\n",
    "\n",
    "        mel1true_y = prepareSpectogram(mel1true_y)\n",
    "        mel1y_pred = prepareSpectogram(mel1y_pred)\n",
    "        mel2true_y = prepareSpectogram(mel2true_y)\n",
    "        mel2y_pred = prepareSpectogram(mel2y_pred)\n",
    "\n",
    "        min_db = 1\n",
    "\n",
    "        dif1 = (mel1true_y - mel1y_pred)**2\n",
    "        dif2 = (mel2true_y - mel2y_pred)**2\n",
    "        dif1 = dif1.clamp(min=min_db) - min_db\n",
    "        dif2 = dif2.clamp(min=min_db) - min_db\n",
    "\n",
    "        dif1 = (dif1**2).mean() * 3\n",
    "        dif2 = (dif2**2).mean() * 3\n",
    "        return dif1 + dif2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.custom_loss = CustomLoss()\n",
    "        self.mel_loss = MelspectogramLoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        #customLoss = self.custom_loss(y_pred, true_y)*1\n",
    "        prop = (((y_pred**2) / ((true_y**2)+0.00001)) - 1)**2\n",
    "        max_limit = 10000\n",
    "        prop = torch.clamp(prop, max=max_limit) \n",
    "        customLoss = prop.mean() * 0.001\n",
    "        melLoss = self.mel_loss(y_pred, true_y)*0.02\n",
    "        l1Loss =  self.l1_loss(y_pred, true_y)*100\n",
    "        return  melLoss, customLoss, l1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., 25., 16., 25., 25., 25.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1., -5., 4., 5., 5., 5.])**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Tq6IXvQW3AYR"
   },
   "outputs": [],
   "source": [
    "class WaveNet(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels, learning_rate):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = CombinedLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.ValLoss = 0\n",
    "        self.TrainLoss = 0\n",
    "        self.epochNumberVal = 0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "    \n",
    "    def change_loss_function(self, loss_fun):\n",
    "        self.loss_fun = loss_fun\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "        \n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "        output = torch.sum(skip_connections, dim=0)\n",
    "\n",
    "\n",
    "        output = self.densnet(output)\n",
    "        return output#.transpose(1, 2).contiguous()\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, lr=0.001):\n",
    "        learning_rate = self.learning_rate\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # compute loss\n",
    "        lossMel, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel + lossAud + customLoss\n",
    "        writer.add_scalar(\"Loss audio\", lossAud, batch_idx)\n",
    "        writer.add_scalar(\"Loss proportion\", customLoss, batch_idx)\n",
    "        writer.add_scalar(\"Loss melspect\", lossMel, batch_idx)\n",
    "        writer.add_scalars(\"Loss total\", {'train':loss,\n",
    "                                'validation':self.ValLoss\n",
    "                                }, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('wave_loss', lossAud, prog_bar=True)\n",
    "        self.log('mel_loss', lossMel, prog_bar=True)\n",
    "        self.log('prop_loss', customLoss, prog_bar=True)\n",
    "        self.log('val_loss', self.ValLoss, prog_bar=True)\n",
    "        self.log('total_loss', loss, prog_bar=True)\n",
    "        self.epochNumberVal =  self.epochNumberVal +1\n",
    "        rand = random.random()\n",
    "        if(rand<0.05):\n",
    "            torch.save(model, modeloNombre)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Save audio clips to TensorBoard\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            audio_clip = y_pred[i].cpu().numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_{self.epochNumberVal}', audio_clip, global_step=batch_idx, sample_rate=16000)\n",
    "        # compute loss\n",
    "        lossMel, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel + lossAud + customLoss\n",
    "        self.ValLoss = loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"pruebaAudios\")\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WGDcsGXnFUcj"
   },
   "outputs": [],
   "source": [
    "modeloNombre = directoryBase+'/wavenet/modelos/wavenetReal/wavenetSacaRuidoBlanco19_2_1_128Layers.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveform, label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "pIsVpbLrPM9V",
    "outputId": "07fcd081-8dd6-4769-b561-06a53db8f16e"
   },
   "outputs": [],
   "source": [
    "#Audio(data=waveform.cpu()[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "WY-A43_oPodH",
    "outputId": "a71d199d-6faf-4100-b97a-48f904d61aec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Audio(data=label.cpu()[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def size_of_tensor(a):\n",
    "    return sys.getsizeof(a) + torch.numel(a)*a.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(wavenetModel.named_parameters()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(layer_size=18, stack_size=2, in_channels=1, res_channels=128, learning_rate=0.01)\n",
    "#model = torch.load(modeloNombre, map_location=torch.device('cuda'))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure_optimizers(lr=0.0005)\n",
    "loss_fun = CombinedLoss()\n",
    "model.change_loss_function(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "traindataset = AudioCleaningDataset(directoryBase+'/wavenet/CSV/audio_train.csv', directoryBase+'/extra/audiosDivididos', directoryBase+'/extra/audiosDivididos/ruidosPocos/', directoryBase+\"/extra/audiosDivididos/reverbPocos\", maxRuido=0.005, fixedInterval=False)\n",
    "traindataloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "valdataset = AudioCleaningDataset(directoryBase+'/wavenet/CSV/audio_valMicro.csv', directoryBase+'/extra/audiosDivididos', directoryBase+'/extra/audiosDivididos/ruidosPocos/', directoryBase+\"/extra/audiosDivididos/reverbPocos\", maxRuido=0.005, fixedInterval=True)\n",
    "valdataloader = DataLoader(valdataset, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | causal    | CausalConv1d  | 256   \n",
      "1 | res_stack | ResidualStack | 0     \n",
      "2 | densnet   | DensNet       | 8     \n",
      "3 | loss_fun  | CombinedLoss  | 0     \n",
      "--------------------------------------------\n",
      "264       Trainable params\n",
      "0         Non-trainable params\n",
      "264       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dda5e0244a546eb9a3258f639ba84bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved. New best score: 16.850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 5.814 >= min_delta = 0.01. New best score: 11.036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 2.678 >= min_delta = 0.01. New best score: 8.358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 1.659 >= min_delta = 0.01. New best score: 6.699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 2.756 >= min_delta = 0.01. New best score: 3.942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 0.510 >= min_delta = 0.01. New best score: 3.432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved by 0.557 >= min_delta = 0.01. New best score: 2.875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24f7786a6d84363852a4a64d0425b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=0.01, patience=1000, verbose=True, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1,\n",
    "                     max_epochs=1000,\n",
    "                     log_every_n_steps=1,\n",
    "                     callbacks=[early_stop_callback],\n",
    "                    accumulate_grad_batches=6\n",
    "                     ,val_check_interval=10  # Perform validation every 10 training steps\n",
    ")\n",
    "trainer.fit(model=model, train_dataloaders=traindataloader\n",
    "            , val_dataloaders=valdataloader\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, modeloNombre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VIMwEY86ggBR"
   },
   "outputs": [],
   "source": [
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5GPj8DOOQpeF"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, true_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))\n\u001b[1;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m true_y \u001b[38;5;241m=\u001b[39m true_y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "X, true_y = next(iter(dataloader))\n",
    "X = X.to(device)\n",
    "true_y = true_y.to(device)\n",
    "model = model.to(device)\n",
    "wdet = model(X).detach()\n",
    "predicho = wdet.cpu()\n",
    "X = X.cpu()\n",
    "true_y = true_y.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2Qif9uIhlmj"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrhzwWcJheSq"
   },
   "outputs": [],
   "source": [
    "loss_fn = CombinedLoss()\n",
    "loss1, loss2, loss3 = loss_fn(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "print(\"error de prediccion \" + str(loss1 +  loss2+ loss3))\n",
    "\n",
    "\n",
    "loss4, loss5, loss6 = loss_fn(X[audioAnalizar], true_y[audioAnalizar])\n",
    "print(\"error sin hacer nada \" + str(  loss4+loss5+loss6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlCnWvlYfSwN"
   },
   "outputs": [],
   "source": [
    "Audio(data=X.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-vUnlapfP2D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(data=predicho.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK2257kufdbA"
   },
   "outputs": [],
   "source": [
    "Audio(data=true_y.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN6XNTbfj_Gs"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(onda2)), (onda2.numpy()-onda.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL7qTgs0lqRd"
   },
   "outputs": [],
   "source": [
    "pos = 0\n",
    "delta = 32000\n",
    "onda = predicho[audioAnalizar][0][pos:pos+delta]\n",
    "onda2 = true_y[audioAnalizar][0][pos:pos+delta]\n",
    "onda3 = X[audioAnalizar][0][pos:pos+delta]\n",
    "\n",
    "sns.lineplot(x=range(0,len(onda2)), y=onda2.numpy(), label=\"Real\")\n",
    "#sns.lineplot(x=range(0,len(onda3)), y=(onda3), label=\"Sucia\")\n",
    "sns.lineplot(x=range(0,len(onda)), y=onda.numpy(), label=\"Predicha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/home/afridman/wavenet/CSV/newCSV/audiosTest.csv')\n",
    "\n",
    "# Take a sample (in this case, let's take 10 random rows)\n",
    "sample_df = df.sample(n=100)\n",
    "\n",
    "# Save the sample to a new CSV file\n",
    "sample_df.to_csv('/home/afridman/wavenet/CSV/newCSV/audiosMicroTest.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onda2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onda3.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onda.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(onda2.numpy()-onda.numpy()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1FaXu6Hu0eg"
   },
   "outputs": [],
   "source": [
    "  mel1true_y = mel_transform1(true_y.to('cuda'))\n",
    "  mel1y_pred = mel_transform1(predicho.to('cuda'))\n",
    "  mel1sucio = mel_transform1(X.to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skeKllcatulH"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db = transformsaudio.AmplitudeToDB()(mel1true_y).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram verdadero\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyC5VWOcvoAo"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db2 = transformsaudio.AmplitudeToDB()(mel1y_pred).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db2[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram predicho\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZoMFkMFk9XK"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db3 = transformsaudio.AmplitudeToDB()(mel1sucio).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db3[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram sucio\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melsp = MelspectogramLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel1true_y, mel1y_pred, dif1, dif2 = melsp(predicho, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel1true_y[0][0].cpu().detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram verdadero\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel1y_pred[0][0].cpu().detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram predicho\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(dif1[0][0].cpu().detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram diferencia\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvV-BuT9uRTr"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT2gRb26Jrqj"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiurqNrVJgsi"
   },
   "outputs": [],
   "source": [
    "for audioAnalizar in opin:\n",
    "  sf.write('sucio'+str(audioAnalizar)+'.wav', np.ravel(X[audioAnalizar]), 16000, 'PCM_24')\n",
    "  sf.write('original'+str(audioAnalizar)+'.wav', np.ravel(true_y[audioAnalizar]), 16000, 'PCM_24')\n",
    "  sf.write('limpiado'+str(audioAnalizar)+'.wav', np.ravel(predicho[audioAnalizar][0]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5WezPQKzV0x"
   },
   "outputs": [],
   "source": [
    "sf.write('sucio2'+str(audioAnalizar)+'.wav', np.ravel(X[audioAnalizar]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pURARsOjsobo"
   },
   "outputs": [],
   "source": [
    "sf.write('limpiadoEco2'+str(audioAnalizar)+'.wav', np.ravel(predicho[audioAnalizar][0]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGT7A427sper"
   },
   "outputs": [],
   "source": [
    "sf.write('originalSinTocarWavenetReplica2'+str(audioAnalizar)+'.wav', np.ravel(true_y[audioAnalizar]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykFaePRHfwBX"
   },
   "outputs": [],
   "source": [
    "#dataloader give 1 example\n",
    "ini = time.time()\n",
    "for waveform, label in dataloader:\n",
    "    print(waveform.shape)\n",
    "    print(label.shape)\n",
    "    break\n",
    "fin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w66tjFfrsqf2"
   },
   "outputs": [],
   "source": [
    "out = modelPanoramico(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xak0MXc8u2Tq"
   },
   "outputs": [],
   "source": [
    "nuevow = WaveNet( layer_size=18, stack_size=2, in_channels=1, res_channels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZtmnActyucZ"
   },
   "outputs": [],
   "source": [
    "#nuevow.load_state_dict(torch.load(modeloNombre), map_location=torch.device('cpu'))\n",
    "nuevow = torch.load(modeloNombre, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJl69TmxhhUR"
   },
   "outputs": [],
   "source": [
    "predicho = nuevow(waveform).detach().cpu()\n",
    "waveform = waveform.cpu().detach()\n",
    "true_y = label.cpu()\n",
    "X = waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xiyE7QZiHmF"
   },
   "outputs": [],
   "source": [
    "lossSpect1, lossSpect2 = melspectogramLoss(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss1 = loss_fn(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss = loss1 + lossSpect1 + lossSpect2\n",
    "print(\"error de prediccion \" + str(loss.item()))\n",
    "\n",
    "lossSpect1t, lossSpect2t = melspectogramLoss(waveform[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2 = loss_fn(waveform[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2t = loss2 + lossSpect1t + lossSpect2t\n",
    "\n",
    "print(\"error sin hacer nada \" + str(loss2t.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BlKChZoagdq"
   },
   "outputs": [],
   "source": [
    "class WaveNetPanoramico(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNetPanoramico, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = nn.MSELoss()\n",
    "        inp = layer_size*stack_size+res_channels+1\n",
    "        inp2 = math.floor(inp/2)\n",
    "        self.convFinal1 = nn.Conv1d(in_channels=layer_size*stack_size+res_channels+1,\n",
    "                                   out_channels=inp2,\n",
    "                                   kernel_size=15, stride=1,\n",
    "                                   dilation=1, padding=7, bias=True).to(device)\n",
    "        self.convFinal2 = nn.Conv1d(in_channels=inp2,\n",
    "                                   out_channels=1,\n",
    "                                   kernel_size=15, stride=1,\n",
    "                                   dilation=1, padding=7, bias=True).to(device)\n",
    "\n",
    "        self.convFinal3 = nn.Conv1d(in_channels=1,\n",
    "                                   out_channels=1,\n",
    "                                   kernel_size=101, stride=1,\n",
    "                                   dilation=1, padding=50, bias=False).to(device)\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "        copiaConvolucionFinal = x.clone()\n",
    "\n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "\n",
    "\n",
    "        #en vez de sumar skip connections hago convolucion\n",
    "        #print(skip_connections.shape)\n",
    "        #output = torch.sum(skip_connections, dim=0)\n",
    "        skip_connections_squeezed = skip_connections.squeeze()\n",
    "        # Check the shape\n",
    "        # Swap the first two dimensions\n",
    "        skip_connections_squeezed = skip_connections_squeezed.transpose(0, 1)\n",
    "        output = torch.cat((output, copiaConvolucionFinal), dim=1)\n",
    "\n",
    "        output = torch.cat((output, skip_connections_squeezed), dim=1)\n",
    "\n",
    "        output = self.convFinal1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.convFinal2(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.convFinal3(output)\n",
    "\n",
    "        #output = self.densnet(output)\n",
    "\n",
    "        return output#.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cig2EffMohfH"
   },
   "outputs": [],
   "source": [
    "class PostNetSimple(pl.LightningModule):\n",
    "  def __init__(self, layers=12):\n",
    "    super(PostNetSimple, self).__init__()\n",
    "    self.convInicial = nn.Conv1d(in_channels=1,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=33, stride=1,\n",
    "                            dilation=1, padding=16, bias=True).to(\"cuda\")\n",
    "    self.totalLayers = layers\n",
    "    self.convs = []\n",
    "    for conv in range(0, layers):\n",
    "      self.convs.append(\n",
    "          nn.Conv1d(in_channels=128,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=33, stride=1,\n",
    "                            dilation=1, padding=16, bias=True).to(\"cuda\")\n",
    "      )\n",
    "\n",
    "    self.tan = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.convInicial(x)\n",
    "    x = self.tan(x)\n",
    "    for conv in self.convs:\n",
    "      x = conv(x)\n",
    "      x = self.tan(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "directoryBase=\"/home/afridman\"\n",
    "# loading the temp.zip and creating a zip object\n",
    "with ZipFile(directoryBase+ '/extra/audiosPaises2.zip', 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path=directoryBase+ '/extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def add_impulse_response(audio_signal, impulse_response):\n",
    "    # Ensure both audio and impulse response have the same device (CPU or GPU)\n",
    "    impulse_response = impulse_response.to(audio_signal.device)\n",
    "    \n",
    "    # Apply the impulse response using 1D convolution\n",
    "    output_audio = F.conv1d(audio_signal.unsqueeze(0).unsqueeze(0), impulse_response.unsqueeze(0).unsqueeze(0))\n",
    "    \n",
    "    return output_audio.squeeze(0).squeeze(0)\n",
    "\n",
    "# Example usage\n",
    "audio_signal = torch.randn(1000)  # Replace with your actual audio data\n",
    "impulse_response = torch.randn(200)  # Replace with your actual impulse response data\n",
    "\n",
    "# Apply the impulse response\n",
    "output_audio = add_impulse_response(audio_signal, impulse_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.5687e+00, -9.3573e+00, -7.3352e-01, -6.8507e+00,  9.0835e+00,\n",
       "         9.5852e+00, -3.5008e+00, -4.0582e+00,  1.5354e+00, -1.5502e+01,\n",
       "        -1.2079e+01,  7.7278e-02,  1.5648e+00,  6.2965e-01,  1.8557e+01,\n",
       "        -1.6554e+00, -1.2635e+01, -5.6840e+00,  1.4760e+01, -3.0606e+00,\n",
       "        -1.7563e+01, -3.2267e+00,  2.6477e+01, -1.8612e+00, -1.2754e+01,\n",
       "        -2.8657e+00, -9.3973e+00,  8.3042e+00, -2.3713e+00, -1.0660e+01,\n",
       "        -1.8093e+01, -3.9224e+00,  5.6275e+00,  1.2004e+01, -8.2134e+00,\n",
       "        -1.2713e+01,  1.3887e+01,  1.9645e+01,  1.3015e+01,  1.6071e+01,\n",
       "        -8.6562e+00, -2.9002e+01, -2.4760e+01,  1.0437e+01, -3.3004e+01,\n",
       "        -3.7859e+00,  2.2630e+00,  1.2608e+01,  5.4650e+00,  1.8472e+01,\n",
       "         2.8007e+01,  1.8272e+01, -1.4786e+00,  1.8029e+01, -1.8204e+01,\n",
       "        -7.2283e+00, -6.4111e+00, -1.4654e+01, -1.0531e+01,  2.9071e+00,\n",
       "        -4.4362e+00,  2.6543e+00,  2.7406e+00,  2.1918e+01,  1.7892e+01,\n",
       "         1.6038e+01, -9.4173e+00,  5.1944e+00, -1.5698e+01, -7.4730e+00,\n",
       "         3.3707e+00, -1.4134e+01,  9.2523e-01, -2.6704e+01, -1.0058e+01,\n",
       "         8.0258e+00,  2.5162e+01, -2.6441e+01, -4.0359e+00, -2.6307e+00,\n",
       "         4.5222e+00,  2.2484e+01,  2.7495e+00,  6.6347e+00, -1.1433e+01,\n",
       "         3.3845e+00,  1.4249e+01, -4.4389e+00, -3.6180e+00, -5.1106e+00,\n",
       "        -3.5966e+00,  9.6393e+00,  1.2570e+01, -1.3239e+01,  2.2779e+01,\n",
       "        -1.1111e+01, -2.5999e+00,  1.0269e+01,  1.0300e+00, -2.6435e+01,\n",
       "        -1.2211e+00,  3.3067e+00, -6.4243e+00,  9.7046e+00,  2.0925e+01,\n",
       "         3.4281e+01, -1.6385e+01, -1.2776e+01, -6.3852e+00, -1.2707e+01,\n",
       "        -2.6322e-01,  1.9268e+00, -2.0208e+01, -1.9371e+01, -8.2907e+00,\n",
       "        -2.0904e+00,  1.5557e+01, -1.6433e+01,  3.9559e+00,  9.5202e+00,\n",
       "         1.0858e+00,  1.3819e+01,  2.5752e+01,  1.1803e+01, -6.2900e-01,\n",
       "        -3.2375e+00, -1.0176e+01, -1.7913e+01,  5.1269e+00, -9.3170e+00,\n",
       "        -1.7412e+00,  3.2384e+00,  1.5241e+01,  4.3435e+01,  3.4556e+00,\n",
       "        -1.8921e+00,  3.3842e+00,  1.8370e-01, -8.0052e-01, -2.2430e+01,\n",
       "        -1.0633e+01, -9.3710e+00, -1.0051e+01, -5.6613e-01, -1.9824e+00,\n",
       "         1.3755e+01,  1.5095e+01, -6.7775e+00, -1.6090e+01,  1.5439e+01,\n",
       "         5.5554e+00,  1.0820e+01,  3.6874e+00, -1.6976e+01, -2.5515e+01,\n",
       "         1.7183e-01,  1.0384e+01, -4.8417e+00, -1.0845e+01,  7.5991e+00,\n",
       "        -1.7293e+01, -3.1211e+00, -1.8318e+00,  3.6014e+01,  7.7880e+00,\n",
       "        -2.8205e+01,  3.7244e+00,  4.3580e+00,  1.6411e+01,  1.1993e+01,\n",
       "         1.6861e+01, -1.7876e+01, -2.4184e+01,  6.5071e+00,  2.3189e+01,\n",
       "         7.8391e+00, -2.3100e+00, -1.8113e+01, -9.5506e+00,  3.3451e+00,\n",
       "        -7.9444e-01,  1.7546e+01,  3.7364e-01, -1.5309e+01, -1.3303e+01,\n",
       "         4.2156e+00,  4.8981e+00,  5.6770e+00,  2.0184e+00,  4.8321e-02,\n",
       "         8.6594e+00,  1.1140e+01,  1.8486e+01, -5.2301e+00, -2.5836e+01,\n",
       "        -1.8553e+01, -9.1573e+00, -2.9093e+00, -1.2404e+01, -7.6636e+00,\n",
       "        -6.7079e+00,  3.0981e+00,  1.1317e+01,  2.1142e+01, -1.2125e+01,\n",
       "        -5.2328e-01, -3.4683e+00, -1.1931e+01, -1.4925e+00,  1.1796e+01,\n",
       "        -9.5838e+00,  1.8442e+01, -1.2465e+01, -2.0474e+01, -1.4083e+00,\n",
       "         7.9320e+00,  6.4542e+00, -3.2449e+01, -2.2094e+01, -6.3843e+00,\n",
       "         9.6304e+00,  3.0839e+01,  1.5312e+01,  3.3008e+00, -2.4387e+00,\n",
       "         1.8523e+01, -2.0964e+00,  1.6092e+01, -8.4916e+00, -6.4065e+00,\n",
       "        -8.1729e+00, -1.6496e+01, -8.9487e+00, -8.7056e+00, -9.3766e+00,\n",
       "        -2.6825e+01,  1.0676e+01,  1.0388e+01, -1.4458e+01,  6.1708e+00,\n",
       "         1.4249e+01, -6.4989e+00, -1.6803e+01,  2.2956e+01,  3.7897e+01,\n",
       "        -1.4084e+01,  8.6787e+00,  1.0768e+01, -1.0856e+01, -6.1213e+00,\n",
       "        -7.3105e+00,  1.4979e+00, -2.4150e+00, -2.6507e+01, -8.1883e-01,\n",
       "         3.0727e+01,  9.7616e+00,  1.4609e+01, -3.3115e-02, -1.5262e+01,\n",
       "         1.7573e+01,  6.1113e+00,  1.3812e+01, -1.1341e+01, -2.0609e+01,\n",
       "         7.3973e-01,  2.9080e-01, -8.8806e+00, -3.8113e-01,  3.5700e+00,\n",
       "         3.4336e+00, -3.2975e+01,  7.3304e+00,  1.8077e+01,  1.5064e+01,\n",
       "         1.3358e+01, -1.0030e+01, -2.0383e+01, -1.0748e+01,  2.2736e+01,\n",
       "         7.3420e+00, -5.4959e+00, -7.1582e+00, -2.7450e+01, -5.4669e+00,\n",
       "         7.1675e+00, -4.7482e+00, -9.1196e+00,  1.3337e+01, -5.6269e+00,\n",
       "        -5.2667e+00,  1.4980e+01,  1.0489e+01,  1.6204e+01,  1.0839e+01,\n",
       "        -5.2698e+00,  1.3542e+01,  1.2544e+01,  5.4652e+00, -2.1754e+01,\n",
       "        -1.5544e+01,  1.7746e+01,  3.6743e+00,  1.7277e+01, -1.9036e+01,\n",
       "        -4.9089e+00,  2.5349e+00,  2.5998e+00,  8.8937e+00,  8.2173e-01,\n",
       "         1.1333e+01, -6.6276e+00, -3.5585e+00, -9.2164e+00,  7.0073e-01,\n",
       "         2.5743e+00, -4.1248e-01,  5.6405e+00,  1.0302e+01,  1.3399e+01,\n",
       "         1.0825e+00, -2.1406e+00,  2.0300e+01,  1.2670e+00,  8.4178e+00,\n",
       "        -4.0289e+00, -7.8814e+00, -2.3331e+01, -1.0149e+01,  1.8153e+01,\n",
       "         1.3772e-01, -3.7785e+00, -8.0447e+00, -1.2555e+01,  6.4897e+00,\n",
       "         6.4847e+00,  8.2343e+00, -2.0217e+01, -3.6397e+00,  5.0425e-01,\n",
       "         1.9537e-01, -5.6196e+00,  7.0724e+00, -4.7760e+00, -1.6940e+00,\n",
       "        -8.8300e+00,  6.0931e+00,  2.9303e+00,  2.7983e+00, -6.3679e+00,\n",
       "         1.0756e+01,  5.1367e+00,  2.2240e-01, -1.8460e+00,  1.2975e+00,\n",
       "         4.3400e+00,  5.0199e+00,  1.6433e+01,  6.1713e+00, -1.0869e+01,\n",
       "        -1.5122e+01,  1.8750e+01,  3.1918e+00, -4.4154e+00,  4.6710e+00,\n",
       "        -1.5410e+01, -1.1721e+01,  4.0540e+00,  3.8640e+00, -7.3667e+00,\n",
       "        -1.8818e+01, -2.7416e+00,  3.4382e+00,  2.0859e+01, -6.5469e+00,\n",
       "         8.6854e+00, -6.9606e+00, -1.2018e+01, -1.3840e+01,  2.5364e+01,\n",
       "         1.8200e+00, -1.7234e+01, -1.9222e+01,  2.1909e+01, -1.6519e+01,\n",
       "         3.8610e+00,  4.0393e+00,  1.3175e+01, -9.9019e+00, -3.1099e+00,\n",
       "         1.4231e+01, -1.6122e+00, -7.7321e+00, -4.6015e+00, -5.7470e+00,\n",
       "        -5.2150e+00,  9.1124e+00,  1.6256e+01, -2.6379e+01,  1.5456e+01,\n",
       "        -2.6172e+01,  1.6599e+00,  9.0637e+00,  8.0483e+00,  3.1154e+00,\n",
       "        -2.8783e+01,  3.5187e+00,  3.2575e+00,  1.2089e+01,  3.5573e+00,\n",
       "        -2.5244e+01,  9.3925e-01,  1.5978e+01,  4.0653e+00,  3.6518e+00,\n",
       "        -6.0197e+00, -8.6636e+00, -1.0654e+01,  1.5012e+01, -2.5606e+00,\n",
       "         4.1579e+00,  5.3271e+00,  1.8582e+01,  9.3885e-01,  4.7185e+00,\n",
       "         9.0258e+00, -1.1947e+01, -1.5747e+01,  1.7906e+00,  2.4618e+01,\n",
       "         2.5809e+01, -1.7051e+01, -2.1746e+00, -3.5802e+01,  1.0994e+01,\n",
       "        -1.0863e+01,  1.1576e+01,  5.0490e+00, -5.6541e+00,  7.5805e+00,\n",
       "         4.5339e+00,  5.6586e+00, -4.2269e+00, -6.3616e+00,  1.2224e+01,\n",
       "         2.2657e+01, -8.8362e+00, -1.5311e+01, -9.7847e+00, -1.3182e+01,\n",
       "        -5.1687e+00,  2.5325e+00, -6.5800e-01,  3.6714e+00,  7.4617e-01,\n",
       "         7.7988e+00,  1.7687e+01,  2.0642e+01, -1.6716e+00, -7.1070e-01,\n",
       "         9.6518e+00,  9.1530e+00, -1.1958e+01,  1.1406e+01, -1.7039e+01,\n",
       "        -2.2613e+01, -3.1330e+01,  2.2450e+01,  7.8230e+00,  1.1618e+01,\n",
       "        -1.6003e+01, -6.7043e+00, -7.0059e+00,  4.2137e+00,  1.7842e+01,\n",
       "         3.1026e+00, -1.2965e+01, -1.8850e+00, -1.5992e+01, -1.2160e+00,\n",
       "         6.1958e+00,  2.9242e+01, -2.0157e+00, -2.3427e+00,  9.2758e+00,\n",
       "         4.0481e-01,  4.5705e+00,  8.4754e+00,  7.0573e-02, -8.8447e+00,\n",
       "         3.7586e+00,  2.2537e+01, -9.9425e-03,  6.7977e+00,  1.4082e+01,\n",
       "         2.5949e+01, -9.3628e+00,  3.8089e+00, -1.8738e+01,  1.7542e+01,\n",
       "         7.2108e+00,  3.7801e+00,  2.4581e+00,  8.1580e+00, -5.9749e+00,\n",
       "        -3.6477e+00,  6.9467e+00, -4.8094e+00, -1.2698e+01, -2.1730e+00,\n",
       "         3.6180e+00, -1.5070e+01, -2.3346e+01, -1.5107e+01,  1.0911e+01,\n",
       "        -1.3957e+01, -1.7343e+01, -1.5193e+01, -2.7111e+00, -2.2678e+01,\n",
       "        -1.2567e+01,  4.8706e+00,  8.5950e+00, -2.0853e+00, -4.0652e+00,\n",
       "         1.4366e+01, -1.9636e+00, -1.1640e+01,  7.3278e+00, -1.3336e+01,\n",
       "        -4.2853e+00, -1.8489e+01,  1.6561e+01, -9.2382e+00, -9.2192e+00,\n",
       "        -7.8307e+00,  1.8832e+01, -1.5530e+01,  1.8090e+01,  1.3458e+01,\n",
       "         8.8060e+00, -1.1856e+01, -4.1505e+00,  7.5114e+00, -9.5171e+00,\n",
       "        -4.5677e+00, -2.7912e+00, -1.1124e+01,  2.5200e+00,  1.2216e+01,\n",
       "         1.8046e+01,  3.4965e+00, -1.2519e+01, -1.1149e+01,  1.0870e+01,\n",
       "        -3.5374e+00,  1.1641e+01,  4.8073e+00,  1.4700e+01, -6.4560e+00,\n",
       "         6.6911e+00,  2.3186e+01,  1.5438e+00,  1.4896e+01, -2.1898e+01,\n",
       "        -1.6284e+01, -1.3498e+01, -7.5317e-01,  8.9219e+00, -2.5464e+01,\n",
       "        -8.0152e+00, -1.2418e+01,  4.4278e-01,  2.0957e+01,  1.8658e+00,\n",
       "        -2.4877e+01,  4.1485e+00, -2.2911e+00,  1.1526e+01,  9.8929e+00,\n",
       "         9.3959e+00,  1.5276e+01, -1.3911e+01, -1.4082e+01,  1.6316e+00,\n",
       "         1.9440e+00, -2.1620e+00,  6.1661e-01, -4.3784e+00,  8.7734e+00,\n",
       "        -3.9705e+00,  1.2659e+01, -4.7570e-01,  1.6209e+01,  1.4939e+01,\n",
       "         9.5793e-01, -9.7457e+00, -2.8667e+01,  8.5405e+00, -1.1296e+01,\n",
       "        -7.8623e-01, -7.0252e+00, -3.6474e+00,  8.8436e+00, -1.6094e+01,\n",
       "         6.8529e+00, -2.2385e+01,  1.5380e+01, -8.7309e+00,  2.3866e+00,\n",
       "        -1.0938e+00,  3.1668e+00, -7.0880e+00, -2.1637e+01,  1.0678e+01,\n",
       "         1.4231e+01,  1.3537e+01, -6.2127e-01, -9.7394e+00, -1.7636e+01,\n",
       "         2.8713e+00,  2.3541e+01, -6.8181e+00, -1.3398e+01,  1.0284e+01,\n",
       "         2.0187e+01,  3.6232e+01, -1.9864e-01,  8.6367e+00,  8.8605e+00,\n",
       "         4.0531e+00, -1.2511e+01,  1.3837e+01,  5.7402e+00, -8.3332e+00,\n",
       "        -6.7758e+00, -2.1038e+01, -1.4412e+01,  6.4341e+00, -8.8911e+00,\n",
       "         7.7291e+00, -7.4106e+00, -2.3835e+01,  1.2884e+01,  1.3104e+01,\n",
       "        -1.9705e+01, -2.8123e+00, -5.0246e+00, -1.4584e+00, -1.0719e+00,\n",
       "        -4.9658e+00,  7.1418e+00,  2.4024e+00, -1.0736e+01,  4.1124e+00,\n",
       "        -7.9628e+00,  3.4846e+00, -2.2865e+00, -6.4288e+00,  1.9788e+01,\n",
       "        -2.0216e+01, -5.7581e+00, -8.1616e+00, -2.3807e+01, -8.9727e+00,\n",
       "         9.7642e+00,  3.8048e+00, -2.7166e+00,  4.5599e+00, -5.2523e+00,\n",
       "        -1.3739e+00, -1.4327e+00,  2.3055e+01,  1.4984e+01,  9.3209e-01,\n",
       "        -1.8612e+01,  8.2955e+00,  1.3698e+01, -7.5021e+00, -5.5980e+00,\n",
       "        -4.6594e+00, -3.3084e+01,  1.8043e+01, -8.0922e+00, -4.7051e+00,\n",
       "        -2.1254e+01, -4.7446e+00,  2.3163e+01,  1.4242e+01,  1.3355e+01,\n",
       "        -1.5883e-01, -4.0100e+00,  1.8226e+01,  2.4142e+00,  2.6081e+00,\n",
       "         1.6315e+01,  2.1004e+01,  2.8858e+00,  8.5721e+00,  1.0459e+01,\n",
       "         2.6236e+00, -2.9566e+00, -4.2614e+00, -9.7582e+00, -1.5099e+01,\n",
       "         2.8876e+00, -1.4775e+00, -8.5678e-02, -6.4101e+00,  1.9041e+01,\n",
       "         1.4492e+00, -6.5903e+00,  1.2928e+01,  7.0861e+00,  2.0939e+00,\n",
       "        -7.6819e+00,  7.2847e+00,  4.5697e+00,  5.2190e+00, -1.3799e+01,\n",
       "         1.7322e+00,  8.6671e+00, -9.0638e-01,  7.1417e+00, -8.4015e+00,\n",
       "         9.2782e+00, -6.3510e+00,  8.7602e+00, -5.2038e+00, -1.9503e+01,\n",
       "        -3.2844e+00,  9.4991e+00,  1.5501e+01,  7.9301e-01,  1.3719e+01,\n",
       "         2.4649e+00, -2.7502e+00,  4.7875e+00, -3.6441e+00,  1.1749e+00,\n",
       "        -2.1515e+01, -3.0294e+01, -6.9577e+00,  1.0912e+00,  1.1219e+01,\n",
       "         5.6606e+00,  1.7753e+01, -1.0456e+00,  2.9787e+00,  1.7326e+00,\n",
       "         4.5992e+00,  1.1599e+01, -1.2009e+01,  8.2874e+00,  6.1590e+00,\n",
       "        -4.2030e+00, -1.0531e+00,  1.5884e+01,  1.3247e+01,  9.3563e+00,\n",
       "         7.2203e+00,  9.4654e-01, -1.0622e+01, -7.4375e-01, -1.0657e+01,\n",
       "         2.7646e+00,  2.7483e+01, -1.4244e+00, -1.1976e+01, -1.6669e+01,\n",
       "         1.0910e+01, -1.2307e+01, -1.5512e+00,  1.3380e+00, -1.3146e+01,\n",
       "         1.2824e+01,  2.8933e+01, -7.6693e+00, -9.1303e+00,  8.8695e+00,\n",
       "         4.3943e-01, -1.6757e+01,  1.1701e+01,  1.5724e+01, -2.9983e+00,\n",
       "        -3.7445e+01, -1.8899e+00,  1.3993e+00, -3.6507e+00,  9.8466e-01,\n",
       "         9.0766e+00, -2.6021e+01,  3.4279e-01, -3.4512e+00,  1.1097e+01,\n",
       "         1.0343e+01, -1.1563e+01, -6.8909e+00,  6.1655e+00, -2.2304e+00,\n",
       "         2.3447e+00])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando archivo entrenamiento\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejecutando archivo entrenamiento\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.fft\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as transformsaudio\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import soundfile as sf\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.functional as Fa\n",
    "\n",
    "import math\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "from torchmetrics.audio import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.audio import ShortTimeObjectiveIntelligibility\n",
    "from torchmetrics.audio import SpeechReverberationModulationEnergyRatio\n",
    "from torchmetrics.audio import SignalNoiseRatio\n",
    "\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from PIL import Image\n",
    "from prettytable import PrettyTable\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 0.2\n",
    "add_impulse_response = True\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioCleaningDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, noise_dir, noise_csv, reverb_dir, target_length=inputSize, maxRuido=0.001, fixedInterval=False, snr=snr, ir_on = add_impulse_response, nivelDificultadReverb =1):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.noise_df = pd.read_csv(noise_csv)\n",
    "        self.noise_dir = noise_dir\n",
    "        self.reverb_dir = reverb_dir\n",
    "        self.target_length = target_length\n",
    "        self.resampleo = transformsaudio.Resample(orig_freq=48000, new_freq=16000)  # Resampling\n",
    "        self.resampleoIR = transformsaudio.Resample(orig_freq=32000, new_freq=16000)  # Resampling\n",
    "        self.ir = ir_on\n",
    "        self.maxRuido = maxRuido\n",
    "        self.snr = snr\n",
    "        \n",
    "        self.reverb_files = os.listdir(reverb_dir)\n",
    "        self.nivelDificultadReverb = nivelDificultadReverb\n",
    "\n",
    "        self.conv= transformsaudio.Convolve(mode=\"same\")\n",
    "        self.fixedInterval = fixedInterval\n",
    "        \n",
    "    def activate_ir(self):\n",
    "        self.ir=True\n",
    "    def change_max_ruido(self, maxRuidopar):\n",
    "        self.maxRuido = maxRuidopar\n",
    "    def change_snr(self, snrpar):\n",
    "        self.snr = snrpar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_file_name = os.path.join(self.audio_dir, self.dataframe.iloc[idx]['audio_file_name']+\".wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_name)\n",
    "        #start, end = find_most_energetic_interval(audio_file_name)\n",
    "        waveformOriginal = self.resampleo(waveform)\n",
    "        waveformSucia = waveformOriginal.clone()\n",
    "\n",
    "        if(self.ir):\n",
    "            IR_file_name = random.choice(self.reverb_files)\n",
    "            IR_waveform, sample_Rate_IR = torchaudio.load(os.path.join(self.reverb_dir, IR_file_name))\n",
    "            IR_waveform = self.resampleoIR(IR_waveform)#[:inputSize]\n",
    "            convd = self.nivelDificultadReverb * Fa.fftconvolve(waveformSucia, IR_waveform, mode=\"full\")\n",
    "\n",
    "\n",
    "            waveformSucia = waveformSucia + convd[:,:waveformSucia[0].shape[0]]\n",
    "            waveformSuciaMaxAbs = waveformSucia.abs().max()\n",
    "            waveformSucia = waveformSucia / (waveformSuciaMaxAbs*1.5)\n",
    "        \n",
    "        #Me quedo con los primeros 2 seg\n",
    "        if(self.fixedInterval):\n",
    "            waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "            waveformSucia = waveformSucia[:,:self.target_length]\n",
    "        \n",
    "        padding0 = torch.zeros((1, max(self.target_length - waveformOriginal.size(1),1)))\n",
    "        padding1 = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding0), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding1), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "        # Load a random noise file\n",
    "        noise_file_name = random.choice(self.noise_df[\"file_name_with_directory\"])\n",
    "        \n",
    "        noise_waveform, sample_Rate_ruido = torchaudio.load(os.path.join(self.noise_dir, noise_file_name))\n",
    "        # Repeat the noise waveform until it's at least as long as the audio waveform\n",
    "        while noise_waveform.size(1) < waveformSucia.size(1):\n",
    "            noise_waveform = torch.cat((noise_waveform, noise_waveform), dim=1)\n",
    "\n",
    "        # Trim the noise waveform to match the length of the audio waveform\n",
    "        noise_waveform = noise_waveform[:,:waveformSucia.size(1)]\n",
    "\n",
    "        waveformSucia = waveformSucia + (noise_waveform * self.snr)\n",
    "        whitenoise = random.uniform(self.maxRuido / 6, self.maxRuido)\n",
    "        waveformSucia = waveformSucia + torch.randn_like(waveformOriginal) * whitenoise\n",
    "        \n",
    "\n",
    "        return 1*waveformSucia, 1*waveformOriginal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCleanerNet(pl.LightningModule):\n",
    "    def __init__(self, inputSize, lr) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = 12\n",
    "\n",
    "        kernelSize = 5\n",
    "        paddingSize = math.floor(kernelSize/2)\n",
    "\n",
    "        self.convs = []\n",
    "        for i in range(0,self.layers,1):\n",
    "          if(i<(self.layers/2 )):\n",
    "            self.convs.append(nn.Conv1d(in_channels=(2**i), out_channels=(2**i), kernel_size=kernelSize, stride=1, dilation=1, padding=2).to(device))\n",
    "            self.convs.append(nn.Conv1d(in_channels=(2**i), out_channels=2**(i+1), kernel_size=kernelSize, stride=1, dilation=2**(i), padding=2*2**(i)).to(device))\n",
    "          else:\n",
    "            self.convs.append(nn.Conv1d(in_channels=1+(2**(self.layers-i)), out_channels=1+(2**(self.layers-i)), kernel_size=kernelSize, stride=1, dilation=1, padding=2).to(device))\n",
    "            self.convs.append(nn.Conv1d(in_channels=1+(2**(self.layers-i)), out_channels=2**(self.layers-i-1), kernel_size=kernelSize, stride=1, dilation=(2**(self.layers-i)), padding=2*(2**(self.layers-i))).to(device))\n",
    "\n",
    "        self.convFinal = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=1, dilation=1, padding=1).to(device)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tan = nn.Tanh()\n",
    "        self.learning_rate = lr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_x = x.view(x.size(0), -1)  # Flatten the original input\n",
    "        original_x = original_x.unsqueeze(1)\n",
    "\n",
    "        for i in range(0,self.layers,1):\n",
    "          if(i>=(self.layers/2 )):\n",
    "            combined_tensor = torch.cat((original_x, x), dim=1)\n",
    "          else:\n",
    "            combined_tensor = x\n",
    "          convActual = self.convs[2*i]\n",
    "          convSig = self.convs[2*i+1]\n",
    "\n",
    "          x = convActual(combined_tensor)\n",
    "          x = self.tan(x)\n",
    "          x = convSig(x)\n",
    "          x = self.tan(x)\n",
    "\n",
    "\n",
    "        combined_tensor = torch.cat((original_x, x), dim=1)\n",
    "\n",
    "        x = self.convFinal(combined_tensor)\n",
    "        #x = self.tan(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self, lr=learning_rate):\n",
    "        learning_rate = self.learning_rate\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # compute loss\n",
    "        lossMel, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel + lossAud\n",
    "        writer.add_scalar(\"Loss audio\", lossAud, batch_idx)\n",
    "        writer.add_scalar(\"Loss melspect\", lossMel, batch_idx)\n",
    "        writer.add_scalar(\"Loss total\", loss, batch_idx)\n",
    "        self.log('wave_loss', lossAud, prog_bar=True)\n",
    "        self.log('mel_loss', lossMel, prog_bar=True)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        rand = random.random()\n",
    "        if(rand<0.05):\n",
    "            torch.save(model, modeloNombre)\n",
    "        return loss\n",
    "    \n",
    "    def val_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fun(y_pred, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super(DilatedCausalConv1d, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(channels, channels,\n",
    "                                    kernel_size=3, stride=1,  # Fixed for WaveNet\n",
    "                                    dilation=dilation,\n",
    "                                    padding=dilation,  # Fixed for WaveNet dilation\n",
    "                                    bias=True)  \n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # padding=1 for same size(length) between input and output for causal convolution\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        # remove last value for causal convolution\n",
    "        return output  #[:, :, :-1]\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation):\n",
    "        \"\"\"\n",
    "        Residual block\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :param dilation:\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dilated = DilatedCausalConv1d(res_channels, dilation=dilation)\n",
    "        self.conv_res = torch.nn.Conv1d(res_channels, res_channels, 1)\n",
    "        self.conv_skip = torch.nn.Conv1d(res_channels, skip_channels, 1)\n",
    "\n",
    "        self.gate_tanh = torch.nn.Tanh()\n",
    "        self.gate_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv_skip = self.conv_skip.to(device)\n",
    "            self.conv_res = self.conv_res.to(device)\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = self.dilated(x)\n",
    "\n",
    "        # PixelCNN gate\n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh * gated_sigmoid\n",
    "\n",
    "        # Residual network\n",
    "        output = self.conv_res(gated)\n",
    "        #output = output[:, :, 0:inputSize]\n",
    "\n",
    "        input_cut = x#[:, :, -output.size(2):]\n",
    "\n",
    "        output += input_cut\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip\n",
    "\n",
    "\n",
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "\n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            # output is the next input\n",
    "            output, skip = res_block(output, skip_size)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "\n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "\n",
    "class DensNet(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DensNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.tan = torch.nn.Tanh()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv1 = self.conv1.to(device)\n",
    "            self.conv2 = self.conv2.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output1 = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class WaveNet(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels, learning_rate):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = CombinedLoss()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.srmr = SpeechReverberationModulationEnergyRatio(16000)\n",
    "        self.wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')\n",
    "        self.stoi = ShortTimeObjectiveIntelligibility(16000, False)\n",
    "        self.snr = SignalNoiseRatio(16000)\n",
    "\n",
    "        self.ValLosses = []\n",
    "        self.ValLoss = 0\n",
    "        self.valMel1Loss = 0\n",
    "        self.valMel2Loss = 0\n",
    "        self.valL1Loss = 0\n",
    "        self.valPropLoss = 0\n",
    "\n",
    "        self.TrainLoss = 0\n",
    "        \n",
    "        \n",
    "        self.epochNumberVal = 0\n",
    "\n",
    "        self.PESQValLoss =0\n",
    "        self.STOIValLoss = 0\n",
    "        self.SRMRValLoss = 0\n",
    "        self.FWSSNRValLoss = 0\n",
    "        \n",
    "        self.PESQValLosses =[]\n",
    "        self.STOIValLosses = []\n",
    "        self.SRMRValLosses = []\n",
    "        self.FWSSNRValLosses = []\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "    \n",
    "    def change_loss_function(self, loss_fun):\n",
    "        self.loss_fun = loss_fun\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "        \n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "        output = torch.sum(skip_connections, dim=0)\n",
    "\n",
    "\n",
    "        output = self.densnet(output)\n",
    "        return output#.transpose(1, 2).contiguous()\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, lr=0.001):\n",
    "        learning_rate = self.learning_rate\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batch_idx = self.epochNumberVal\n",
    "        \n",
    "        if(len(self.ValLosses)>0):\n",
    "            self.ValLoss = np.array(self.ValLosses).mean()\n",
    "            self.ValLosses = []\n",
    "           \n",
    "        \n",
    "        X, y = train_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # compute loss\n",
    "        lossMel1, lossMel2, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel1 * weightOfMelspecLoss1 + lossMel2 * weightOfMelspecLoss2 + lossAud *weightOfL1Loss + customLoss *weightOfCustomLoss\n",
    "        #writer.add_scalar(\"Loss audio\", lossAud, batch_idx)\n",
    "        #writer.add_scalar(\"Loss proportion\", customLoss, batch_idx)\n",
    "        #writer.add_scalar(\"Loss melspect1\", lossMel1, batch_idx)\n",
    "        #writer.add_scalar(\"Loss melspect2\", lossMel2, batch_idx)\n",
    "\n",
    "\n",
    "        trainLosssrmr = self.srmr(y_pred)\n",
    "        trainLossstoi = self.stoi(y_pred, y)\n",
    "        #trainLosspesq = self.wb_pesq(y_pred, y)\n",
    "        trainLosssnr = self.snr(y_pred, y)\n",
    "\n",
    "        \n",
    "\n",
    "        writer.add_scalars(\"STOI\", {'train':trainLossstoi,\n",
    "                        'validation':trainLossstoi\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"SRMR\", {'train':trainLosssrmr,\n",
    "                        'validation':trainLosssrmr\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"SNR\", {'train':trainLosssnr,\n",
    "                        'validation':trainLosssnr\n",
    "                        }, batch_idx)\n",
    "        \n",
    "        #writer.add_scalars(\"PESQ\", {'train':trainLosspesq,\n",
    "        #                                'validation':trainLosspesq\n",
    "        #                               }, batch_idx)\n",
    "        \n",
    "        \n",
    "        writer.add_scalars(\"Loss L1\", {'train':lossAud,\n",
    "                                'validation':self.valL1Loss\n",
    "                                }, batch_idx)\n",
    "        writer.add_scalars(\"Loss prop\", {'train':customLoss,\n",
    "                        'validation':self.valPropLoss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss melspect1\", {'train':lossMel1,\n",
    "                        'validation':self.valMel1Loss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss melspect2\", {'train':lossMel2,\n",
    "                        'validation':self.valMel2Loss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss total\", {'train':loss,\n",
    "                                'validation':self.ValLoss\n",
    "                                }, batch_idx)\n",
    "        self.log('val_loss', self.ValLoss, prog_bar=True)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('wave_loss', lossAud, prog_bar=True)\n",
    "        #self.log('mel_loss1', lossMel1, prog_bar=True)\n",
    "        #self.log('mel_loss2', lossMel2, prog_bar=True)\n",
    "        #self.log('prop_loss', customLoss, prog_bar=True)\n",
    "        #self.log('val_loss', self.ValLoss, prog_bar=True)\n",
    "        #self.log('total_loss', loss, prog_bar=True)\n",
    "        self.epochNumberVal =  self.epochNumberVal +1\n",
    "        rand = random.random()\n",
    "        if((self.epochNumberVal % saveModelIntervalEpochs) == 0):\n",
    "            torch.save(model, modeloNombre)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        if(batch_idx<howManyAudiosValidationsSave):\n",
    "            #for i in range(X.shape[0]):\n",
    "            audio_clip = X[0].cpu().numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_Sucio', audio_clip, global_step=batch_idx, sample_rate=16000)\n",
    "            audio_clip = y[0].cpu().numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_Original', audio_clip, global_step=batch_idx, sample_rate=16000)\n",
    "            audio_clip = y_pred[0].cpu()#.numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_{self.epochNumberVal}', audio_clip, global_step=batch_idx,sample_rate=16000)\n",
    "            generatePlots(audio_clip[0], y[0].cpu(), X[0].cpu(), batch_idx, self.epochNumberVal)\n",
    "                \n",
    "        # compute loss\n",
    "        lossMel1, lossMel2, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel1*weightOfMelspecLoss1 + lossMel2*weightOfMelspecLoss2 + lossAud*weightOfL1Loss + customLoss*weightOfCustomLoss\n",
    "        self.log('val_loss', loss) \n",
    "        self.ValLosses.append(loss.item())\n",
    "        self.valMel1Loss = lossMel1\n",
    "        self.valMel2Loss = lossMel2\n",
    "        self.valL1Loss = lossAud\n",
    "        self.valPropLoss = customLoss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryBase = \"/home/afridman\"\n",
    "locationTrainFile = \"/wavenet/CSV/newCSV/audiosTrain.csv\"\n",
    "locationValidationFile = \"/wavenet/CSV/newCSV/audiosVal.csv\"\n",
    "maxRuido = 0.005\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = AudioCleaningDataset(directoryBase+locationTrainFile,\n",
    "                                    directoryBase+'/extra/audiosPaises',\n",
    "                                    directoryBase+'/extra/ruidosDivididos',\n",
    "                                    directoryBase+\"/wavenet/CSV/ruido_train.csv\",\n",
    "                                    directoryBase+\"/extra/irDivididos/irtrain\", \n",
    "                                    maxRuido=maxRuido, fixedInterval=False)\n",
    "\n",
    "\n",
    "valdataset = AudioCleaningDataset(directoryBase+locationValidationFile, \n",
    "                                  directoryBase+'/extra/audiosPaises', \n",
    "                                 directoryBase+'/extra/ruidosDivididos',\n",
    "                                  directoryBase+\"/wavenet/CSV/ruido_validation.csv\",\n",
    "                                  directoryBase+\"/extra/irDivididos/irval\",\n",
    "                                  maxRuido=maxRuido, fixedInterval=True)\n",
    "\n",
    "\n",
    "\n",
    "traindataloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(layer_size=10, stack_size=2, in_channels=1, res_channels=128, learning_rate=0.01)\n",
    "#model = torch.load(modeloNombre, map_location=torch.device('cuda'))\n",
    "model = model.to(device)\n",
    "waveform, label = next(iter(traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('/home/afridman/wavenet/CSV/newCSV/audiosTest.csv')\n",
    "\n",
    "# Take a sample (in this case, let's take 10 random rows)\n",
    "sample_df = df.sample(n=100)\n",
    "\n",
    "# Save the sample to a new CSV file\n",
    "sample_df.to_csv('/home/afridman/wavenet/CSV/newCSV/audiosMicroTest.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Define the layers with specified configurations\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=15, stride=1, groups=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=16, out_channels=64, kernel_size=41, stride=4, groups=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=64, out_channels=256, kernel_size=41, stride=4, groups=16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=256, out_channels=1024, kernel_size=41, stride=4, groups=64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=41, stride=4, groups=256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=1024, out_channels=1024, kernel_size=5, stride=1, groups=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=1024, out_channels=1, kernel_size=3, stride=1, groups=1),\n",
    "            \n",
    "            nn.Linear(106, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the Discriminator\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Test with a random input\n",
    "input_tensor = torch.randn(1, 1, 32000)  # Assuming input size is (batch_size, channels, sequence_length)\n",
    "output_tensor = discriminator(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5056, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_tensor[0][0][0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
