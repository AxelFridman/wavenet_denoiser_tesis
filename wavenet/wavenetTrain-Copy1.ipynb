{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgPQ8uFiMWwg",
    "outputId": "58e76746-8757-432e-bfb9-a415371b87fb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/afridman/wavenet']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3248\u001b[0m _run_async \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3251\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3254\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:106\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:92\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 92\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/tempfile.py:291\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/afridman/wavenet']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.fft\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as transformsaudio\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import soundfile as sf\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "writer = SummaryWriter()\n",
    "%load_ext tensorboard\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/afridman/wavenet']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3248\u001b[0m _run_async \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 3251\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3254\u001b[0m         \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/compilerop.py:155\u001b[0m, in \u001b[0;36mCachingCompiler.cache\u001b[0;34m(self, transformed_code, number, raw_code)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     raw_code \u001b[38;5;241m=\u001b[39m transformed_code\n\u001b[0;32m--> 155\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_code_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save the execution count\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename_map[name] \u001b[38;5;241m=\u001b[39m number\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:106\u001b[0m, in \u001b[0;36mXCachingCompiler.get_code_name\u001b[0;34m(self, raw_code, code, number)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_code_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_code, code, number):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the code name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_file_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:92\u001b[0m, in \u001b[0;36mget_file_name\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     name \u001b[38;5;241m=\u001b[39m murmur2_x86(code, get_tmp_hash_seed())\n\u001b[0;32m---> 92\u001b[0m     cell_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_tmp_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_name\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/ipykernel/compiler.py:76\u001b[0m, in \u001b[0;36mget_tmp_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tmp_directory\u001b[39m():\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a temp directory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     tmp_dir \u001b[38;5;241m=\u001b[39m convert_to_long_pathname(\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     pid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_dir \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipykernel_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pid)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/tempfile.py:291\u001b[0m, in \u001b[0;36mgettempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tempdir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m         tempdir \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_tempdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     _once_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/tempfile.py:223\u001b[0m, in \u001b[0;36m_get_default_tempdir\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# no point trying more names in this directory\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(_errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    224\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable temporary directory found in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    225\u001b[0m                         dirlist)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/home/afridman/wavenet']"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "directoryBase = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q2geUymaNvfH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_max_length(tensor1, tensor2):\n",
    "    max_length = max(tensor1.size(1), tensor2.size(1))\n",
    "\n",
    "    pad_tensor1 = torch.nn.functional.pad(tensor1, (0, max_length - tensor1.size(1)))\n",
    "    pad_tensor2 = torch.nn.functional.pad(tensor2, (0, max_length - tensor2.size(1)))\n",
    "\n",
    "    return pad_tensor1, pad_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KRPe4vgOOROy"
   },
   "outputs": [],
   "source": [
    "inputSize = 32000\n",
    "class AudioCleaningDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, noise_dir, reverb_dir, target_length=inputSize, maxRuido=0):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.noise_dir = noise_dir\n",
    "        self.reverb_dir = reverb_dir\n",
    "        self.target_length = target_length\n",
    "        self.resampleo = transformsaudio.Resample(orig_freq=48000, new_freq=16000)  # Resampling\n",
    "        self.resampleoIR = transformsaudio.Resample(orig_freq=32000, new_freq=16000)  # Resampling\n",
    "\n",
    "        self.noise_files = os.listdir(noise_dir) # List all noise files\n",
    "        self.reverb_files = os.listdir(reverb_dir)\n",
    "        self.maxRuido = maxRuido\n",
    "        self.conv= transformsaudio.Convolve(mode=\"same\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_file_name = os.path.join(self.audio_dir, self.dataframe.iloc[idx]['audio_file_name']+\".wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_name)\n",
    "\n",
    "        waveformOriginal = self.resampleo(waveform)\n",
    "\n",
    "        #aca seleccionar\n",
    "        maximoPosible = len(waveformOriginal[0])-self.target_length\n",
    "        if(maximoPosible>1):\n",
    "          comienzoAleatorio = (random.randint(0,maximoPosible))\n",
    "          waveformOriginal = waveformOriginal[:,comienzoAleatorio:comienzoAleatorio+self.target_length]\n",
    "        #print(len(waveformOriginal[0]))\n",
    "\n",
    "\n",
    "        waveformSucia = waveformOriginal.clone()\n",
    "        padding = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "        # Load a random noise file\n",
    "        noise_file_name = random.choice(self.noise_files)\n",
    "        noise_waveform, sample_Rate_ruido = torchaudio.load(os.path.join(self.noise_dir, noise_file_name))\n",
    "        # Repeat the noise waveform until it's at least as long as the audio waveform\n",
    "        while noise_waveform.size(1) < waveformSucia.size(1):\n",
    "            noise_waveform = torch.cat((noise_waveform, noise_waveform), dim=1)\n",
    "\n",
    "        # Trim the noise waveform to match the length of the audio waveform\n",
    "        noise_waveform = noise_waveform[:,:waveformSucia.size(1)]\n",
    "\n",
    "        # Add noise with a random signal-to-noise ratio between 0.01 and 0.1\n",
    "        snr = random.uniform(0.001, 0.01) #random.uniform(0.01, 0.1)\n",
    "\n",
    "        # LE SACO EL RUIDO SUCIO PARA ACELERAR APRENDIZAJE !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #waveformSucia = waveformSucia + noise_waveform * snr\n",
    "        whitenoise = random.uniform(0.0, self.maxRuido)\n",
    "        waveformSucia = waveformSucia + torch.randn_like(waveformOriginal) * whitenoise\n",
    "        return 1*waveformSucia, 1*waveformOriginal\n",
    "\n",
    "\n",
    "        IR_file_name = random.choice(self.reverb_files)\n",
    "        IR_waveform, sample_Rate_IR = torchaudio.load(os.path.join(self.reverb_dir, IR_file_name))\n",
    "        IR_waveform = self.resampleoIR(IR_waveform)\n",
    "\n",
    "        # Normalize impulse response\n",
    "        normalized_ir = IR_waveform / (IR_waveform.abs().max())\n",
    "\n",
    "        # Perform convolution\n",
    "        padded_signal, padded_filter = pad_to_max_length(waveformSucia, normalized_ir)\n",
    "\n",
    "        # Perform the FFT\n",
    "        fft_signal = torch.fft.fft(padded_signal)\n",
    "        fft_filter = torch.fft.fft(padded_filter)\n",
    "\n",
    "        # Perform the convolution in the frequency domain\n",
    "        fft_result = fft_signal * fft_filter\n",
    "\n",
    "        # Perform the inverse FFT to get the result in the time domain\n",
    "        result = torch.fft.ifft(fft_result)\n",
    "\n",
    "        # The result is complex, take the real part\n",
    "\n",
    "        # LE SACO EL REVERB PARA ACELERAR APRENDIZAJE !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #waveformSucia = result.real\n",
    "\n",
    "        padding = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "       # waveformOriginal = (waveformOriginal - waveformOriginal.mean()) / waveformOriginal.std()\n",
    "       # waveformSucia = (waveformSucia - waveformSucia.mean()) / waveformSucia.std()\n",
    "       # Optional: Normalize convolved audio\n",
    "       # waveformSucia = waveformSucia / (waveformSucia.abs().max())\n",
    "       # waveformOriginal = waveformOriginal / (waveformOriginal.abs().max())\n",
    "\n",
    "        return 1*waveformSucia, 1*waveformOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rqo9d72ZAvve"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural network modules for WaveNet\n",
    "\n",
    "References :\n",
    "    https://arxiv.org/pdf/1609.03499.pdf\n",
    "    https://github.com/ibab/tensorflow-wavenet\n",
    "    https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a\n",
    "    https://github.com/musyoku/wavenet/issues/4\n",
    "\"\"\"\n",
    "\n",
    "class DilatedCausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super(DilatedCausalConv1d, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(channels, channels,\n",
    "                                    kernel_size=2, stride=1,  # Fixed for WaveNet\n",
    "                                    dilation=dilation,\n",
    "                                    padding=dilation,  # Fixed for WaveNet dilation\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # padding=1 for same size(length) between input and output for causal convolution\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=2, stride=1, padding=1,\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        # remove last value for causal convolution\n",
    "        return output[:, :, :-1]\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation):\n",
    "        \"\"\"\n",
    "        Residual block\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :param dilation:\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dilated = DilatedCausalConv1d(res_channels, dilation=dilation)\n",
    "        self.conv_res = torch.nn.Conv1d(res_channels, res_channels, 1)\n",
    "        self.conv_skip = torch.nn.Conv1d(res_channels, skip_channels, 1)\n",
    "\n",
    "        self.gate_tanh = torch.nn.Tanh()\n",
    "        self.gate_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv_skip = self.conv_skip.to(device)\n",
    "            self.conv_res = self.conv_res.to(device)\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = self.dilated(x)\n",
    "\n",
    "        # PixelCNN gate\n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh * gated_sigmoid\n",
    "\n",
    "        # Residual network\n",
    "        output = self.conv_res(gated)\n",
    "        output = output[:, :, 0:inputSize]\n",
    "\n",
    "        input_cut = x#[:, :, -output.size(2):]\n",
    "\n",
    "        output += input_cut\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip\n",
    "\n",
    "\n",
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "\n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            # output is the next input\n",
    "            output, skip = res_block(output, skip_size)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "\n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "\n",
    "class DensNet(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DensNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.tan = torch.nn.Tanh()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv1 = self.conv1.to(device)\n",
    "            self.conv2 = self.conv2.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.tan(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X1LUHnVDXWGz"
   },
   "outputs": [],
   "source": [
    "#loss_fn = nn.L1Loss()\n",
    "#learning_rate = 0.0015\n",
    "#optimizer = torch.optim.AdamW(params=wavenetModel.parameters(), lr=learning_rate)\n",
    "#optimizerPost = torch.optim.AdamW(params=postnetModel.parameters(), lr=learning_rate)\n",
    "\n",
    "mel_transform1 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 2048,\n",
    "                                                n_mels = 120,\n",
    "                                                hop_length = 512).to('cuda')\n",
    "\n",
    "mel_transform2 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 512,\n",
    "                                                n_mels = 80,\n",
    "                                                hop_length = 128).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lOjB-ReSsR-Y"
   },
   "outputs": [],
   "source": [
    "def prepareSpectogram(melspect):\n",
    "    melspect = ((melspect - melspect.min()) / (melspect.max()-melspect.min()) + 0.00000001)\n",
    "    melspect=melspect.log10()\n",
    "    min_db = -6\n",
    "    # Clamp the values in the tensor to be no less than min_db\n",
    "    melspect = melspect.clamp(min=min_db)\n",
    "    return melspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XhoYwzuouEqi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    def melspectogramLoss(y_pred, true_y):\\n\\n      mel1true_y = (mel_transform1(true_y.to('cuda')))\\n      mel1y_pred = (mel_transform1(y_pred.to('cuda')))\\n      mel2true_y = (mel_transform2(true_y.to('cuda')))\\n      mel2y_pred = (mel_transform2(y_pred.to('cuda')))\\n\\n      mel1true_y = prepareSpectogram(mel1true_y)\\n      mel1y_pred = prepareSpectogram(mel1y_pred)\\n      mel2true_y = prepareSpectogram(mel2true_y)\\n      mel2y_pred = prepareSpectogram(mel2y_pred)\\n\\n      dif1 = (mel1true_y - mel1y_pred).abs().mean()\\n      dif2 = (mel2true_y - mel2y_pred).abs().mean()\\n      # Define the minimum dB value\\n\\n\\n      return dif1*15 + dif2*10\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    def melspectogramLoss(y_pred, true_y):\n",
    "\n",
    "      mel1true_y = (mel_transform1(true_y.to('cuda')))\n",
    "      mel1y_pred = (mel_transform1(y_pred.to('cuda')))\n",
    "      mel2true_y = (mel_transform2(true_y.to('cuda')))\n",
    "      mel2y_pred = (mel_transform2(y_pred.to('cuda')))\n",
    "\n",
    "      mel1true_y = prepareSpectogram(mel1true_y)\n",
    "      mel1y_pred = prepareSpectogram(mel1y_pred)\n",
    "      mel2true_y = prepareSpectogram(mel2true_y)\n",
    "      mel2y_pred = prepareSpectogram(mel2y_pred)\n",
    "\n",
    "      dif1 = (mel1true_y - mel1y_pred).abs().mean()\n",
    "      dif2 = (mel2true_y - mel2y_pred).abs().mean()\n",
    "      # Define the minimum dB value\n",
    "\n",
    "\n",
    "      return dif1*15 + dif2*10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MelspectogramLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelspectogramLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        mel1true_y = (mel_transform1(true_y.to('cuda')))\n",
    "        mel1y_pred = (mel_transform1(y_pred.to('cuda')))\n",
    "        mel2true_y = (mel_transform2(true_y.to('cuda')))\n",
    "        mel2y_pred = (mel_transform2(y_pred.to('cuda')))\n",
    "\n",
    "        mel1true_y = prepareSpectogram(mel1true_y)\n",
    "        mel1y_pred = prepareSpectogram(mel1y_pred)\n",
    "        mel2true_y = prepareSpectogram(mel2true_y)\n",
    "        mel2y_pred = prepareSpectogram(mel2y_pred)\n",
    "\n",
    "        dif1 = (mel1true_y - mel1y_pred).abs().mean()\n",
    "        dif2 = (mel2true_y - mel2y_pred).abs().mean()\n",
    "\n",
    "        return dif1*15 + dif2*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.mel_loss = MelspectogramLoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        return self.mel_loss(y_pred, true_y) + self.l1_loss(y_pred, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Tq6IXvQW3AYR"
   },
   "outputs": [],
   "source": [
    "class WaveNet(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = CombinedLoss()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "\n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "        output = torch.sum(skip_connections, dim=0)\n",
    "\n",
    "\n",
    "        output = self.densnet(output)\n",
    "\n",
    "        return output#.transpose(1, 2).contiguous()\n",
    "    \n",
    "    def configure_optimizers(self, lr=learning_rate):\n",
    "        learning_rate = lr\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fun(y_pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        rand = random.random()\n",
    "        if(rand<0.05):\n",
    "            torch.save(model, modeloNombre)\n",
    "        return loss\n",
    "    \n",
    "    def val_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fun(y_pred, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WGDcsGXnFUcj"
   },
   "outputs": [],
   "source": [
    "modeloNombre = directoryBase+'/modelos/wavenetReal/wavenetReplicaAudio.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveform, label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "pIsVpbLrPM9V",
    "outputId": "07fcd081-8dd6-4769-b561-06a53db8f16e"
   },
   "outputs": [],
   "source": [
    "#Audio(data=waveform.cpu()[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "WY-A43_oPodH",
    "outputId": "a71d199d-6faf-4100-b97a-48f904d61aec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Audio(data=label.cpu()[0], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def size_of_tensor(a):\n",
    "    return sys.getsizeof(a) + torch.numel(a)*a.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(wavenetModel.named_parameters()).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(layer_size=18, stack_size=2, in_channels=1, res_channels=128)\n",
    "model = torch.load(modeloNombre, map_location=torch.device('cuda'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | causal    | CausalConv1d  | 256   \n",
      "1 | res_stack | ResidualStack | 0     \n",
      "2 | densnet   | DensNet       | 8     \n",
      "3 | loss_fun  | MSELoss       | 0     \n",
      "--------------------------------------------\n",
      "264       Trainable params\n",
      "0         Non-trainable params\n",
      "264       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd18bdf59ca4189a81e37ef8490fac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved. New best score: 0.000\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "dataset = AudioCleaningDataset(directoryBase+'/CSV/audiosBastantes.csv', directoryBase+'/audiosDivididos/', directoryBase+'/audiosDivididos/ruidosPocos/', directoryBase+\"/audiosDivididos/reverbPocos\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "# training\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=0.4, patience=1, verbose=True, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1, log_every_n_steps=2, callbacks=[early_stop_callback])\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, modeloNombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "15oUUOrUA7iI"
   },
   "outputs": [],
   "source": [
    "wavenetModel = WaveNet( layer_size=18, stack_size=2, in_channels=1, res_channels=128)\n",
    "\n",
    "wavenetModel = torch.load(modeloNombre, map_location=torch.device('cuda'))\n",
    "wavenetModel = wavenetModel.to(device)\n",
    "#.load_state_dict(torch.load(modeloNombre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'waveform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m size_of_tensor(\u001b[43mwaveform\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'waveform' is not defined"
     ]
    }
   ],
   "source": [
    "size_of_tensor(waveform)/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "a/1024/1024/1024, r/1024/1024/1024, t/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#wavenetModel(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeVHJosnaZpO"
   },
   "outputs": [],
   "source": [
    "#modelPanoramico = WaveNetPanoramico(layer_size=18, stack_size=2, in_channels=1, res_channels=128)\n",
    "#modelPanoramico.load_state_dict(torch.load(modeloNombre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEqE-CwR3wd0"
   },
   "outputs": [],
   "source": [
    "#modeloPostnetNombre = '/content/drive/MyDrive/tesisPabloAxel/wavenet/modelos/wavenetReal/PostNetSimple12capas.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBCJQJZirDdu"
   },
   "outputs": [],
   "source": [
    "#postnetModel = PostNetSimple()\n",
    "#postnetModel.load_state_dict(torch.load(modeloPostnetNombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiZBEDdYQRKz"
   },
   "outputs": [],
   "source": [
    "#wavenetModel = modelCargado\n",
    "#WaveNet( layer_size=15, stack_size=2, in_channels=1, res_channels=128)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOUQ1vuzXb1P"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "dataset = AudioCleaningDataset(directoryBase+'/CSV/audiosBastantes.csv', directoryBase+'/audiosDivididos/', directoryBase+'/audiosDivididos/ruidosPocos/', directoryBase+\"/audiosDivididos/reverbPocos\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6vDXnP1uYzR",
    "outputId": "98c31831-c786-4910-ae38-684dc391a121"
   },
   "outputs": [],
   "source": [
    "melspectogramLoss(waveform, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HvLxmjhxmMK"
   },
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWROtyB-ghGW"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "OJIN73ueXi0i",
    "outputId": "ad31960c-a887-43f2-e4a8-533cb006f1d9"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    avgLossOfBatch = []\n",
    "    for i, (X, true_y) in enumerate(dataloader, 0):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        y_pred = wavenetModel(X.to(device))\n",
    "        #y_pred_postnet = postnetModel(y_pred)\n",
    "        # calc losses\n",
    "\n",
    "        lossAudio = loss_fn(y_pred, true_y) * 100\n",
    "        lossSpect1, lossSpect2 = melspectogramLoss(y_pred, true_y)\n",
    "\n",
    "\n",
    "        loss = lossAudio + lossSpect1 + lossSpect2\n",
    "        #lossSpectPost1, lossSpectPost2 = melspectogramLoss(y_pred_postnet, true_y)\n",
    "        #lossPostnet = loss_fn(y_pred_postnet, true_y) * 10 + lossSpectPost1 + lossSpectPost2\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        #lossPostnet.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        #optimizerPost.step()\n",
    "\n",
    "        avgLossOfBatch.append(loss.item())\n",
    "        xs = 2917 * epoch + i\n",
    "        writer.add_scalar(\"Loss audio\", lossAudio, xs)\n",
    "        writer.add_scalar(\"Loss spect1\", lossSpect1, xs)\n",
    "        writer.add_scalar(\"Loss spect2\", lossSpect2, xs)\n",
    "        writer.add_scalar(\"Loss spect all\", lossSpect1 + lossSpect2, xs)\n",
    "\n",
    "        writer.add_scalar(\"Loss total\", loss, i)\n",
    "\n",
    "        if((i%1)==0):\n",
    "          print(str(round(100*(i)/(2917/batch_size),2)) + \"% del epoch \" + str(epoch))\n",
    "          prom = np.array(avgLossOfBatch).mean()\n",
    "          losses.append(prom)\n",
    "          avgLossOfBatch = []\n",
    "          #print(prom)\n",
    "          print(\"Perdida de wavenet: \")\n",
    "          print(loss)\n",
    "          print()\n",
    "          print(lossAudio)\n",
    "          print(lossSpect1)\n",
    "          print(lossSpect2)\n",
    "          #torch.save(wavenetModel.state_dict(), modeloNombre)\n",
    "          torch.save(wavenetModel, modeloNombre)\n",
    "            #torch.save(postnetModel.state_dict(), modeloPostnetNombre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHz_D5qdRLr1"
   },
   "outputs": [],
   "source": [
    "torch.save(wavenetModel, modeloNombre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqdDiK-4E3sk"
   },
   "outputs": [],
   "source": [
    "13 % 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIMwEY86ggBR"
   },
   "outputs": [],
   "source": [
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GPj8DOOQpeF"
   },
   "outputs": [],
   "source": [
    "#predicho = wavenetModel(X).detach().to(\"cuda\")\n",
    "X = X.to(device)\n",
    "true_y = true_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LMbjlLqZ5py"
   },
   "outputs": [],
   "source": [
    "wdet = wavenetModel(X).detach()\n",
    "predicho = wdet.cpu()\n",
    "X = X.cpu()\n",
    "true_y = true_y.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpQ2WayMJIPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1HLJvJxI2M0"
   },
   "outputs": [],
   "source": [
    "opin = [1,2,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2Qif9uIhlmj"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrhzwWcJheSq"
   },
   "outputs": [],
   "source": [
    "lossSpect1, lossSpect2 = melspectogramLoss(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss1 = loss_fn(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss = loss1 + lossSpect1 + lossSpect2\n",
    "print(\"error de prediccion \" + str(loss.item()))\n",
    "\n",
    "\n",
    "lossSpect1t, lossSpect2t = melspectogramLoss(X[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2 = loss_fn(X[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2t = loss2 + lossSpect1t + lossSpect2t\n",
    "\n",
    "print(\"error sin hacer nada \" + str(loss2t.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlCnWvlYfSwN"
   },
   "outputs": [],
   "source": [
    "Audio(data=X.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-vUnlapfP2D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(data=predicho.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJwkKaDf0YmV"
   },
   "outputs": [],
   "source": [
    "Audio(data=predichoPost.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK2257kufdbA"
   },
   "outputs": [],
   "source": [
    "Audio(data=true_y.cpu()[audioAnalizar], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN6XNTbfj_Gs"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isnan(true_y[audioAnalizar]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(onda2)), (onda2.numpy()-onda.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,len(onda)), onda.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL7qTgs0lqRd"
   },
   "outputs": [],
   "source": [
    "pos = 0\n",
    "delta = 32000\n",
    "onda = predicho[audioAnalizar][0][pos:pos+delta]\n",
    "onda2 = true_y[audioAnalizar][0][pos:pos+delta]\n",
    "#onda3 = X[audioAnalizar][0][pos:pos+delta]\n",
    "\n",
    "sns.lineplot(x=range(0,len(onda2)), y=onda2.numpy(), label=\"Real\")\n",
    "#sns.lineplot(x=range(0,len(onda3)), y=(onda3), label=\"Sucia\")\n",
    "sns.lineplot(x=range(0,len(onda)), y=onda.numpy(), label=\"Predicha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1FaXu6Hu0eg"
   },
   "outputs": [],
   "source": [
    "  mel1true_y = mel_transform1(true_y.to('cuda'))\n",
    "  mel1y_pred = mel_transform1(predicho.to('cuda'))\n",
    "  mel1sucio = mel_transform1(X.to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skeKllcatulH"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db = transformsaudio.AmplitudeToDB()(mel1true_y).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram verdadero\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyC5VWOcvoAo"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db2 = transformsaudio.AmplitudeToDB()(mel1y_pred).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db2[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram predicho\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZoMFkMFk9XK"
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_db3 = transformsaudio.AmplitudeToDB()(mel1sucio).cpu()\n",
    "\n",
    "\n",
    "# Display the mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram_db3[0][0].detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel Spectrogram sucio\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvV-BuT9uRTr"
   },
   "outputs": [],
   "source": [
    "audioAnalizar = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT2gRb26Jrqj"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiurqNrVJgsi"
   },
   "outputs": [],
   "source": [
    "for audioAnalizar in opin:\n",
    "  sf.write('sucio'+str(audioAnalizar)+'.wav', np.ravel(X[audioAnalizar]), 16000, 'PCM_24')\n",
    "  sf.write('original'+str(audioAnalizar)+'.wav', np.ravel(true_y[audioAnalizar]), 16000, 'PCM_24')\n",
    "  sf.write('limpiado'+str(audioAnalizar)+'.wav', np.ravel(predicho[audioAnalizar][0]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5WezPQKzV0x"
   },
   "outputs": [],
   "source": [
    "sf.write('sucio'+str(audioAnalizar)+'.wav', np.ravel(X[audioAnalizar]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pURARsOjsobo"
   },
   "outputs": [],
   "source": [
    "sf.write('limpiadoEco'+str(audioAnalizar)+'.wav', np.ravel(predicho[audioAnalizar][0]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGT7A427sper"
   },
   "outputs": [],
   "source": [
    "sf.write('originalSinTocarWavenetReplica'+str(audioAnalizar)+'.wav', np.ravel(true_y[audioAnalizar]), 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykFaePRHfwBX"
   },
   "outputs": [],
   "source": [
    "#dataloader give 1 example\n",
    "ini = time.time()\n",
    "for waveform, label in dataloader:\n",
    "    print(waveform.shape)\n",
    "    print(label.shape)\n",
    "    break\n",
    "fin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w66tjFfrsqf2"
   },
   "outputs": [],
   "source": [
    "out = modelPanoramico(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xak0MXc8u2Tq"
   },
   "outputs": [],
   "source": [
    "nuevow = WaveNet( layer_size=18, stack_size=2, in_channels=1, res_channels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZtmnActyucZ"
   },
   "outputs": [],
   "source": [
    "#nuevow.load_state_dict(torch.load(modeloNombre), map_location=torch.device('cpu'))\n",
    "nuevow = torch.load(modeloNombre, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJl69TmxhhUR"
   },
   "outputs": [],
   "source": [
    "predicho = nuevow(waveform).detach().cpu()\n",
    "waveform = waveform.cpu().detach()\n",
    "true_y = label.cpu()\n",
    "X = waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xiyE7QZiHmF"
   },
   "outputs": [],
   "source": [
    "lossSpect1, lossSpect2 = melspectogramLoss(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss1 = loss_fn(predicho[audioAnalizar], true_y[audioAnalizar])\n",
    "loss = loss1 + lossSpect1 + lossSpect2\n",
    "print(\"error de prediccion \" + str(loss.item()))\n",
    "\n",
    "lossSpect1t, lossSpect2t = melspectogramLoss(waveform[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2 = loss_fn(waveform[audioAnalizar], true_y[audioAnalizar])\n",
    "loss2t = loss2 + lossSpect1t + lossSpect2t\n",
    "\n",
    "print(\"error sin hacer nada \" + str(loss2t.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BlKChZoagdq"
   },
   "outputs": [],
   "source": [
    "class WaveNetPanoramico(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNetPanoramico, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = nn.MSELoss()\n",
    "        inp = layer_size*stack_size+res_channels+1\n",
    "        inp2 = math.floor(inp/2)\n",
    "        self.convFinal1 = nn.Conv1d(in_channels=layer_size*stack_size+res_channels+1,\n",
    "                                   out_channels=inp2,\n",
    "                                   kernel_size=15, stride=1,\n",
    "                                   dilation=1, padding=7, bias=True).to(device)\n",
    "        self.convFinal2 = nn.Conv1d(in_channels=inp2,\n",
    "                                   out_channels=1,\n",
    "                                   kernel_size=15, stride=1,\n",
    "                                   dilation=1, padding=7, bias=True).to(device)\n",
    "\n",
    "        self.convFinal3 = nn.Conv1d(in_channels=1,\n",
    "                                   out_channels=1,\n",
    "                                   kernel_size=101, stride=1,\n",
    "                                   dilation=1, padding=50, bias=False).to(device)\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "        copiaConvolucionFinal = x.clone()\n",
    "\n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "\n",
    "\n",
    "        #en vez de sumar skip connections hago convolucion\n",
    "        #print(skip_connections.shape)\n",
    "        #output = torch.sum(skip_connections, dim=0)\n",
    "        skip_connections_squeezed = skip_connections.squeeze()\n",
    "        # Check the shape\n",
    "        # Swap the first two dimensions\n",
    "        skip_connections_squeezed = skip_connections_squeezed.transpose(0, 1)\n",
    "        output = torch.cat((output, copiaConvolucionFinal), dim=1)\n",
    "\n",
    "        output = torch.cat((output, skip_connections_squeezed), dim=1)\n",
    "\n",
    "        output = self.convFinal1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.convFinal2(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.convFinal3(output)\n",
    "\n",
    "        #output = self.densnet(output)\n",
    "\n",
    "        return output#.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cig2EffMohfH"
   },
   "outputs": [],
   "source": [
    "class PostNetSimple(pl.LightningModule):\n",
    "  def __init__(self, layers=12):\n",
    "    super(PostNetSimple, self).__init__()\n",
    "    self.convInicial = nn.Conv1d(in_channels=1,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=33, stride=1,\n",
    "                            dilation=1, padding=16, bias=True).to(\"cuda\")\n",
    "    self.totalLayers = layers\n",
    "    self.convs = []\n",
    "    for conv in range(0, layers):\n",
    "      self.convs.append(\n",
    "          nn.Conv1d(in_channels=128,\n",
    "                            out_channels=128,\n",
    "                            kernel_size=33, stride=1,\n",
    "                            dilation=1, padding=16, bias=True).to(\"cuda\")\n",
    "      )\n",
    "    self.convFinal = nn.Conv1d(in_channels=129,\n",
    "                            out_channels=1,\n",
    "                            kernel_size=33, stride=1,\n",
    "                            dilation=1, padding=16, bias=True).to(\"cuda\")\n",
    "    self.tan = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    xCopia = x.clone()\n",
    "    x = self.convInicial(x)\n",
    "    x = self.tan(x)\n",
    "    for conv in self.convs:\n",
    "      x = conv(x)\n",
    "      x = self.tan(x)\n",
    "    x = torch.cat((x, xCopia), dim=1)\n",
    "    x = self.convFinal(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando archivo entrenamiento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 21:07:38.774916: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-15 21:07:38.774940: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-15 21:07:38.774961: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-15 21:07:38.779900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 21:07:39.428877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input archivo: \n",
      "\n",
      "Importadas todas las librerias\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejecutando archivo entrenamiento\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.fft\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as transformsaudio\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import soundfile as sf\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.functional as Fa\n",
    "\n",
    "import math\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "from torchmetrics.audio import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.audio import ShortTimeObjectiveIntelligibility\n",
    "from torchmetrics.audio import SpeechReverberationModulationEnergyRatio\n",
    "from torchmetrics.audio import SignalNoiseRatio\n",
    "\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from PIL import Image\n",
    "from prettytable import PrettyTable\n",
    "import json\n",
    "\n",
    "\n",
    "print(\"Input archivo: \")\n",
    "print()\n",
    "nombreJson = \"paramsRuns/\"+\"params1Stack15Layers.json\"#str(input())\n",
    "with open(nombreJson, 'r') as file:\n",
    "    params = json.load(file)\n",
    "\n",
    "device = params[\"device\"]\n",
    "directoryBase = params[\"directoryBase\"]\n",
    "inputSize = params[\"inputSize\"]\n",
    "maxRuido = params[\"maxRuido\"]\n",
    "snr = params[\"snr\"]\n",
    "max_epochs = params[\"max_epochs\"]\n",
    "log_every_n_steps = params[\"log_every_n_steps\"]\n",
    "accumulate_grad_batches = params[\"accumulate_grad_batches\"]\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "layer_size = params[\"layer_size\"]\n",
    "stack_size = params[\"stack_size\"]\n",
    "in_channels = params[\"in_channels\"]\n",
    "res_channels = params[\"res_channels\"]\n",
    "useSavedModel = params[\"useSavedModel\"]\n",
    "saveModelIntervalEpochs = params[\"saveModelIntervalEpochs\"]\n",
    "val_check_interval = params[\"val_check_interval\"]\n",
    "min_delta = params[\"min_delta\"]\n",
    "patience = params[\"patience\"]\n",
    "locationTrainFile = params[\"locationTrainFile\"]\n",
    "locationValidationFile = params[\"locationValidationFile\"]\n",
    "howManyAudiosValidationsSave = params[\"howManyAudiosValidationsSave\"]\n",
    "weightOfMelspecLoss1 = params[\"weightOfMelspecLoss1\"]\n",
    "weightOfMelspecLoss2 = params[\"weightOfMelspecLoss2\"]\n",
    "weightOfL1Loss = params[\"weightOfL1Loss\"]\n",
    "weightOfCustomLoss = params[\"weightOfCustomLoss\"]\n",
    "nameOfRun = nombreJson.split(\".\")[0] + params[\"nameOfRun\"]\n",
    "add_impulse_response = params[\"add_impulse_response\"]\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "print(\"Importadas todas las librerias\")\n",
    "\n",
    "\n",
    "def pad_to_max_length(tensor1, tensor2):\n",
    "    max_length = max(tensor1.size(1), tensor2.size(1))\n",
    "\n",
    "    pad_tensor1 = torch.nn.functional.pad(tensor1, (0, max_length - tensor1.size(1)))\n",
    "    pad_tensor2 = torch.nn.functional.pad(tensor2, (0, max_length - tensor2.size(1)))\n",
    "\n",
    "    return pad_tensor1, pad_tensor2\n",
    "def normalizeAudio(wave):\n",
    "    wave = (wave - wave.mean())\n",
    "    waveAbs = max(abs(wave))\n",
    "    wave = wave / (waveAbs)\n",
    "    return wave\n",
    "\n",
    "class AudioCleaningDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, noise_dir, noise_csv, reverb_dir, target_length=inputSize, maxRuido=0.001, fixedInterval=False, snr=snr, ir_on = add_impulse_response):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.noise_df = pd.read_csv(noise_csv)\n",
    "        self.noise_dir = noise_dir\n",
    "        self.reverb_dir = reverb_dir\n",
    "        self.target_length = target_length\n",
    "        self.resampleo = transformsaudio.Resample(orig_freq=48000, new_freq=16000)  # Resampling\n",
    "        self.resampleoIR = transformsaudio.Resample(orig_freq=32000, new_freq=16000)  # Resampling\n",
    "        self.ir = ir_on\n",
    "        self.maxRuido = maxRuido\n",
    "        self.snr = snr\n",
    "        \n",
    "        self.reverb_files = os.listdir(reverb_dir)\n",
    "        self.conv= transformsaudio.Convolve(mode=\"same\")\n",
    "        self.fixedInterval = fixedInterval\n",
    "        \n",
    "    def activate_ir(self):\n",
    "        self.ir=True\n",
    "    def change_max_ruido(self, maxRuidopar):\n",
    "        self.maxRuido = maxRuidopar\n",
    "    def change_snr(self, snrpar):\n",
    "        self.snr = snrpar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_file_name = os.path.join(self.audio_dir, self.dataframe.iloc[idx]['audio_file_name']+\".wav\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_name)\n",
    "        #start, end = find_most_energetic_interval(audio_file_name)\n",
    "        waveformOriginal = self.resampleo(waveform)\n",
    "        waveformSucia = waveformOriginal.clone()\n",
    "\n",
    "        if(self.ir):\n",
    "            IR_file_name = random.choice(self.reverb_files)\n",
    "            IR_waveform, sample_Rate_IR = torchaudio.load(os.path.join(self.reverb_dir, IR_file_name))\n",
    "            IR_waveform = self.resampleoIR(IR_waveform)#[:inputSize]\n",
    "            \n",
    "            waveformSucia = Fa.fftconvolve(waveformSucia, IR_waveform, mode=\"full\")\n",
    "            waveformSuciaMaxAbs = waveformSucia.abs().max()\n",
    "            waveformSucia = waveformSucia / (waveformSuciaMaxAbs*1.5)\n",
    "        \n",
    "        #Me quedo con los primeros 2 seg\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "        \n",
    "        padding0 = torch.zeros((1, max(self.target_length - waveformOriginal.size(1),1)))\n",
    "        padding1 = torch.zeros((1, max(self.target_length - waveformSucia.size(1),1)))\n",
    "\n",
    "        waveformOriginal = torch.cat((waveformOriginal, padding0), dim=1)\n",
    "        waveformSucia = torch.cat((waveformSucia, padding1), dim=1)\n",
    "\n",
    "        waveformOriginal = waveformOriginal[:,:self.target_length]\n",
    "        waveformSucia = waveformSucia[:,:self.target_length]\n",
    "\n",
    "        # Load a random noise file\n",
    "        noise_file_name = random.choice(self.noise_df[\"file_name_with_directory\"])\n",
    "        \n",
    "        noise_waveform, sample_Rate_ruido = torchaudio.load(os.path.join(self.noise_dir, noise_file_name))\n",
    "        # Repeat the noise waveform until it's at least as long as the audio waveform\n",
    "        while noise_waveform.size(1) < waveformSucia.size(1):\n",
    "            noise_waveform = torch.cat((noise_waveform, noise_waveform), dim=1)\n",
    "\n",
    "        # Trim the noise waveform to match the length of the audio waveform\n",
    "        noise_waveform = noise_waveform[:,:waveformSucia.size(1)]\n",
    "\n",
    "        waveformSucia = waveformSucia + (noise_waveform * self.snr)\n",
    "        whitenoise = random.uniform(self.maxRuido / 6, self.maxRuido)\n",
    "        waveformSucia = waveformSucia + torch.randn_like(waveformOriginal) * whitenoise\n",
    "        \n",
    "\n",
    "        return 1*waveformSucia, 1*waveformOriginal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DilatedCausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, channels, dilation=1):\n",
    "        super(DilatedCausalConv1d, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv1d(channels, channels,\n",
    "                                    kernel_size=2, stride=1,  # Fixed for WaveNet\n",
    "                                    dilation=dilation,\n",
    "                                    padding=dilation,  # Fixed for WaveNet dilation\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalConv1d(torch.nn.Module):\n",
    "    \"\"\"Causal Convolution for WaveNet\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # padding=1 for same size(length) between input and output for causal convolution\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=2, stride=1, padding=1,\n",
    "                                    bias=False)  # Fixed for WaveNet but not sure\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv = self.conv.to(device)\n",
    "\n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "\n",
    "        # remove last value for causal convolution\n",
    "        return output[:, :, :-1]\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, dilation):\n",
    "        \"\"\"\n",
    "        Residual block\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :param dilation:\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dilated = DilatedCausalConv1d(res_channels, dilation=dilation)\n",
    "        self.conv_res = torch.nn.Conv1d(res_channels, res_channels, 1)\n",
    "        self.conv_skip = torch.nn.Conv1d(res_channels, skip_channels, 1)\n",
    "\n",
    "        self.gate_tanh = torch.nn.Tanh()\n",
    "        self.gate_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv_skip = self.conv_skip.to(device)\n",
    "            self.conv_res = self.conv_res.to(device)\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = self.dilated(x)\n",
    "\n",
    "        # PixelCNN gate\n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh * gated_sigmoid\n",
    "\n",
    "        # Residual network\n",
    "        output = self.conv_res(gated)\n",
    "        output = output[:, :, 0:inputSize]\n",
    "\n",
    "        input_cut = x#[:, :, -output.size(2):]\n",
    "\n",
    "        output += input_cut\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip\n",
    "\n",
    "\n",
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "\n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks\n",
    "\n",
    "    def forward(self, x, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            # output is the next input\n",
    "            output, skip = res_block(output, skip_size)\n",
    "\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "\n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "\n",
    "class DensNet(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DensNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.tan = torch.nn.Tanh()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.conv1 = self.conv1.to(device)\n",
    "            self.conv2 = self.conv2.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output1 = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mel_transform1 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 2048,\n",
    "                                                n_mels = 120,\n",
    "                                                hop_length = 512).to(device)\n",
    "\n",
    "mel_transform2 = transformsaudio.MelSpectrogram(sample_rate = 16000,\n",
    "                                               n_fft = 512,\n",
    "                                                n_mels = 80,\n",
    "                                                hop_length = 128).to(device)\n",
    "\"\"\"\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, true):\n",
    "        loss = 0\n",
    "\n",
    "        for p, t in zip(predicted, true):\n",
    "            print(t[0].shape)\n",
    "            print(t[0])\n",
    "            if(abs(t)<0.00001):\n",
    "                t = 0.00001\n",
    "            loss += (((p**2) / (t**2)) - 1)**2\n",
    "\n",
    "        return loss\n",
    "\"\"\"\n",
    "def prepareSpectogram(melspect):\n",
    "    melspect = (melspect - melspect.min() + 0.00000001) #/ (melspect.max()-melspect.min()) + 0.00000001)\n",
    "    melspect=melspect.log10()\n",
    "    return melspect\n",
    "\n",
    "class MelspectogramLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MelspectogramLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        mel1true_y = (mel_transform1(true_y.to('cuda')))\n",
    "        mel1y_pred = (mel_transform1(y_pred.to('cuda')))\n",
    "        mel2true_y = (mel_transform2(true_y.to('cuda')))\n",
    "        mel2y_pred = (mel_transform2(y_pred.to('cuda')))\n",
    "\n",
    "        mel1true_y = prepareSpectogram(mel1true_y)\n",
    "        mel1y_pred = prepareSpectogram(mel1y_pred)\n",
    "        mel2true_y = prepareSpectogram(mel2true_y)\n",
    "        mel2y_pred = prepareSpectogram(mel2y_pred)\n",
    "\n",
    "        min_db = 1\n",
    "\n",
    "        dif1 = (mel1true_y - mel1y_pred)**2\n",
    "        dif2 = (mel2true_y - mel2y_pred)**2\n",
    "        dif1 = dif1.clamp(min=min_db) - min_db\n",
    "        dif2 = dif2.clamp(min=min_db) - min_db\n",
    "\n",
    "        dif1 = (dif1**2).mean()\n",
    "        dif2 = (dif2**2).mean()\n",
    "        return dif1 , dif2\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        #self.custom_loss = CustomLoss()\n",
    "        self.mel_loss = MelspectogramLoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, y_pred, true_y):\n",
    "        #customLoss = self.custom_loss(y_pred, true_y)*1\n",
    "        prop = (((y_pred**2) / ((true_y**2)+0.00001)) - 1)**2\n",
    "        max_limit = 10000\n",
    "        prop = torch.clamp(prop, max=max_limit) \n",
    "        customLoss = prop.mean()\n",
    "        melLoss1, melLoss2 = self.mel_loss(y_pred, true_y)\n",
    "        l1Loss =  self.l1_loss(y_pred, true_y)\n",
    "        return  melLoss1, melLoss2, customLoss, l1Loss\n",
    "\n",
    "def prepareSpectogram2(melsp, name, batch_idx, epoch):\n",
    "    mel_spectrogram_db = transformsaudio.AmplitudeToDB()(melsp).cpu()\n",
    "    # Display the mel spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_spectrogram_db.detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.title(f'melspec{batch_idx}_{epoch}_{name}')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Mel Frequency Bin\")\n",
    "    # Save the figure as an image\n",
    "    figure_path = f'melspec_{batch_idx}_{epoch}.png'\n",
    "    plt.savefig(figure_path)\n",
    "    plt.close()\n",
    "    # Open the saved image\n",
    "    with open(figure_path, 'rb') as img_file:\n",
    "        image_data = img_file.read()\n",
    "\n",
    "    # Convert the image data to a PyTorch tensor\n",
    "    image_tensor = torch.from_numpy(plt.imread(figure_path)).permute(2, 0, 1)\n",
    "\n",
    "    # Add the image to TensorBoard\n",
    "    writer.add_image(f'melspec{batch_idx}_{epoch}_{name}', image_tensor, global_step=epoch)\n",
    "\n",
    "    # Remove the saved image file after it's logged to TensorBoard (optional)\n",
    "    os.remove(figure_path)\n",
    "def generateWaveformPlot(predictedAudio, realAudio, noisyAudio, batch_idx, epoch):\n",
    "\n",
    "    \n",
    "    predictedAudio=predictedAudio.numpy().flatten()\n",
    "    realAudio=realAudio.numpy().flatten()\n",
    "    noisyAudio=noisyAudio.numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "\n",
    "    # Plot original audio\n",
    "    sns.lineplot(x=range(0, len(realAudio)), y=realAudio, label=\"Original\")\n",
    "\n",
    "    # Plot noisy audio\n",
    "    sns.lineplot(x=range(0, len(noisyAudio)), y=noisyAudio, label=\"Ruidoso\")\n",
    "\n",
    "    # Plot predicted audio\n",
    "    sns.lineplot(x=range(0, len(predictedAudio)), y=predictedAudio, label=\"Predicho\")\n",
    "\n",
    "    plt.title(f'Waveform_{batch_idx}_{epoch}')\n",
    "\n",
    "    # Save the figure as an image\n",
    "    figure_path = f'waveform_{batch_idx}_{epoch}.png'\n",
    "    plt.savefig(figure_path)\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    # Open the saved image\n",
    "    with open(figure_path, 'rb') as img_file:\n",
    "        image_data = img_file.read()\n",
    "\n",
    "    # Convert the image data to a PyTorch tensor\n",
    "    image_tensor = torch.from_numpy(plt.imread(figure_path)).permute(2, 0, 1)\n",
    "\n",
    "    # Add the image to TensorBoard\n",
    "    writer.add_image(f'waveform_{batch_idx}_{epoch}', image_tensor, global_step=epoch)\n",
    "\n",
    "    # Remove the saved image file after it's logged to TensorBoard (optional)\n",
    "    os.remove(figure_path)\n",
    "\n",
    "    \n",
    "def generatePlots(predictedAudio, realAudio, noisyAudio, batch_idx, epoch):\n",
    "    generateWaveformPlot(predictedAudio, realAudio, noisyAudio, batch_idx, epoch)\n",
    "    mel1true_y = mel_transform1(realAudio.to('cuda'))\n",
    "    mel1y_pred = mel_transform1(predictedAudio.to('cuda'))\n",
    "    mel1sucio = mel_transform1(noisyAudio.to('cuda'))\n",
    "    prepareSpectogram2(mel1y_pred,\"PredichoPrecicionFrec\", batch_idx, epoch)\n",
    "    prepareSpectogram2(mel1sucio[0],\"RuidosoPrecicionFrec\", batch_idx, epoch)\n",
    "    prepareSpectogram2(mel1true_y[0],\"OriginalPrecicionFrec\", batch_idx, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class WaveNet(pl.LightningModule):\n",
    "    def __init__(self, layer_size, stack_size, in_channels, res_channels, learning_rate):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param in_channels: number of channels for input data. skip channel is same as input channel\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.causal = CausalConv1d(in_channels, res_channels)\n",
    "\n",
    "        self.res_stack = ResidualStack(layer_size, stack_size, res_channels, in_channels)\n",
    "\n",
    "        self.densnet = DensNet(in_channels)\n",
    "\n",
    "        self.loss_fun = CombinedLoss()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.srmr = SpeechReverberationModulationEnergyRatio(16000)\n",
    "        self.wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')\n",
    "        self.stoi = ShortTimeObjectiveIntelligibility(16000, False)\n",
    "        self.snr = SignalNoiseRatio(16000)\n",
    "\n",
    "        self.ValLosses = []\n",
    "        self.ValLoss = 0\n",
    "        self.valMel1Loss = 0\n",
    "        self.valMel2Loss = 0\n",
    "        self.valL1Loss = 0\n",
    "        self.valPropLoss = 0\n",
    "\n",
    "        self.TrainLoss = 0\n",
    "        \n",
    "        \n",
    "        self.epochNumberVal = 0\n",
    "\n",
    "        self.PESQValLoss =0\n",
    "        self.STOIValLoss = 0\n",
    "        self.SRMRValLoss = 0\n",
    "        self.FWSSNRValLoss = 0\n",
    "        \n",
    "        self.PESQValLosses =[]\n",
    "        self.STOIValLosses = []\n",
    "        self.SRMRValLosses = []\n",
    "        self.FWSSNRValLosses = []\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(0, layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(layers)\n",
    "\n",
    "        return int(num_receptive_fields)\n",
    "    \n",
    "    def change_loss_function(self, loss_fun):\n",
    "        self.loss_fun = loss_fun\n",
    "\n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "\n",
    "        #self.check_input_size(x, output_size)\n",
    "\n",
    "        return inputSize\n",
    "\n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise InputSizeError(int(x.size(2)), self.receptive_fields, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        output = x#.transpose(1, 2)\n",
    "        \n",
    "        output_size = self.calc_output_size(output)\n",
    "\n",
    "        output = self.causal(output)\n",
    "\n",
    "        skip_connections = self.res_stack(output, output_size)\n",
    "\n",
    "        output = torch.sum(skip_connections, dim=0)\n",
    "\n",
    "\n",
    "        output = self.densnet(output)\n",
    "        return output#.transpose(1, 2).contiguous()\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, lr=0.001):\n",
    "        learning_rate = self.learning_rate\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \n",
    "        if(len(self.ValLosses)>0):\n",
    "            self.ValLoss = np.array(self.ValLosses).mean()\n",
    "            self.ValLosses = []\n",
    "           \n",
    "        \n",
    "        X, y = train_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # compute loss\n",
    "        lossMel1, lossMel2, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel1 * weightOfMelspecLoss1 + lossMel2 * weightOfMelspecLoss2 + lossAud *weightOfL1Loss + customLoss *weightOfCustomLoss\n",
    "        #writer.add_scalar(\"Loss audio\", lossAud, batch_idx)\n",
    "        #writer.add_scalar(\"Loss proportion\", customLoss, batch_idx)\n",
    "        #writer.add_scalar(\"Loss melspect1\", lossMel1, batch_idx)\n",
    "        #writer.add_scalar(\"Loss melspect2\", lossMel2, batch_idx)\n",
    "\n",
    "\n",
    "        trainLosssrmr = self.srmr(y_pred)\n",
    "        trainLossstoi = self.stoi(y_pred, y)\n",
    "        #trainLosspesq = self.wb_pesq(y_pred, y)\n",
    "        trainLosssnr = self.snr(y_pred, y)\n",
    "\n",
    "        \n",
    "\n",
    "        writer.add_scalars(\"STOI\", {'train':trainLossstoi,\n",
    "                        'validation':trainLossstoi\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"SRMR\", {'train':trainLosssrmr,\n",
    "                        'validation':trainLosssrmr\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"SNR\", {'train':trainLosssnr,\n",
    "                        'validation':trainLosssnr\n",
    "                        }, batch_idx)\n",
    "        \n",
    "        #writer.add_scalars(\"PESQ\", {'train':trainLosspesq,\n",
    "        #                                'validation':trainLosspesq\n",
    "        #                               }, batch_idx)\n",
    "        \n",
    "        \n",
    "        writer.add_scalars(\"Loss L1\", {'train':lossAud,\n",
    "                                'validation':self.valL1Loss\n",
    "                                }, batch_idx)\n",
    "        writer.add_scalars(\"Loss prop\", {'train':customLoss,\n",
    "                        'validation':self.valPropLoss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss melspect1\", {'train':lossMel1,\n",
    "                        'validation':self.valMel1Loss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss melspect2\", {'train':lossMel2,\n",
    "                        'validation':self.valMel2Loss\n",
    "                        }, batch_idx)\n",
    "        writer.add_scalars(\"Loss total\", {'train':loss,\n",
    "                                'validation':self.ValLoss\n",
    "                                }, batch_idx)\n",
    "        self.log('val_loss', self.ValLoss, prog_bar=True)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('train_loss', loss, prog_bar=True)\n",
    "        #self.log('wave_loss', lossAud, prog_bar=True)\n",
    "        #self.log('mel_loss1', lossMel1, prog_bar=True)\n",
    "        #self.log('mel_loss2', lossMel2, prog_bar=True)\n",
    "        #self.log('prop_loss', customLoss, prog_bar=True)\n",
    "        #self.log('val_loss', self.ValLoss, prog_bar=True)\n",
    "        #self.log('total_loss', loss, prog_bar=True)\n",
    "        self.epochNumberVal =  self.epochNumberVal +1\n",
    "        rand = random.random()\n",
    "        if((self.epochNumberVal % saveModelIntervalEpochs) == 0):\n",
    "            torch.save(model, modeloNombre)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        if(batch_idx<howManyAudiosValidationsSave):\n",
    "            #for i in range(X.shape[0]):\n",
    "            audio_clip = X[0].cpu().numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_Sucio', audio_clip, global_step=batch_idx, sample_rate=16000)\n",
    "            audio_clip = y[0].cpu().numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_Original', audio_clip, global_step=batch_idx, sample_rate=16000)\n",
    "            audio_clip = y_pred[0].cpu()#.numpy()\n",
    "            writer.add_audio(f'audio_clip_{batch_idx}_{self.epochNumberVal}', audio_clip, global_step=batch_idx,sample_rate=16000)\n",
    "            generatePlots(audio_clip[0], y[0].cpu(), X[0].cpu(), batch_idx, self.epochNumberVal)\n",
    "                \n",
    "        # compute loss\n",
    "        lossMel1, lossMel2, customLoss, lossAud = self.loss_fun(y_pred, y)\n",
    "        loss = lossMel1*weightOfMelspecLoss1 + lossMel2*weightOfMelspecLoss2 + lossAud*weightOfL1Loss + customLoss*weightOfCustomLoss\n",
    "        self.log('val_loss', loss) \n",
    "        self.ValLosses.append(loss.item())\n",
    "        self.valMel1Loss = lossMel1\n",
    "        self.valMel2Loss = lossMel2\n",
    "        self.valL1Loss = lossAud\n",
    "        self.valPropLoss = customLoss\n",
    "        return loss\n",
    "    \n",
    "class DynamicNoiseCallback(pl.Callback):\n",
    "    def __init__(self, traindataset, valdataset, noise_level, max_loss, min_loss):\n",
    "        self.trainDataset = traindataset\n",
    "        self.valDataset = valdataset\n",
    "        self.noise_level = noise_level\n",
    "        self.max_loss = max_loss\n",
    "        self.min_loss = min_loss\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics['val_loss']\n",
    "        \n",
    "        # Adjust noise level based on loss\n",
    "        if (val_loss >= self.min_loss) and (val_loss < self.max_loss):\n",
    "            print(\"val_loss esta entre: \" + str(self.min_loss) + \" y \" +str(self.max_loss))\n",
    "            new_noise_level = self.noise_level\n",
    "\n",
    "            # Set the new noise level in the dataset\n",
    "            self.trainDataset.change_max_ruido(new_noise_level)\n",
    "            self.valDataset.change_max_ruido(new_noise_level)\n",
    "\n",
    "class IrCallback(pl.Callback):\n",
    "    def __init__(self, trainDataset, valDataset, max_loss, min_loss):\n",
    "        self.trainDataset = trainDataset\n",
    "        self.valDataset = valDataset\n",
    "        self.max_loss = max_loss\n",
    "        self.min_loss = min_loss\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics['val_loss']\n",
    "        \n",
    "        # Adjust noise level based on loss\n",
    "        if (val_loss >= self.min_loss) and (val_loss < self.max_loss):\n",
    "            print(\"active ir\")\n",
    "            self.trainDataset.activate_ir()\n",
    "            self.valDataset.activate_ir()            \n",
    "\n",
    "class DynamicSNRCallback(pl.Callback):\n",
    "    def __init__(self, trainDataset, valDataset, noise_level, max_loss, min_loss):\n",
    "        self.trainDataset = trainDataset\n",
    "        self.valDataset = valDataset\n",
    "        self.noise_level = noise_level\n",
    "        self.max_loss = max_loss\n",
    "        self.min_loss = min_loss\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics['val_loss']\n",
    "        \n",
    "        # Adjust noise level based on loss\n",
    "        if (val_loss >= self.min_loss) and (val_loss < self.max_loss):\n",
    "            print(\"val_loss esta entre: \" + str(self.min_loss) + \" y \" +str(self.max_loss))\n",
    "            new_noise_level = self.noise_level\n",
    "\n",
    "            # Set the new noise level in the dataset\n",
    "            self.trainDataset.change_snr(new_noise_level)\n",
    "            self.valDataset.change_snr(new_noise_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(comment=nameOfRun)\n",
    "#%load_ext tensorboard\n",
    "modeloNombre = directoryBase+'/wavenet/modelos/'+nameOfRun+'.pth'\n",
    "\n",
    "model = WaveNet(layer_size=layer_size, stack_size=stack_size, in_channels=in_channels, res_channels=res_channels, learning_rate=learning_rate)\n",
    "if(useSavedModel):\n",
    "    model = torch.load(modeloNombre, map_location=torch.device('cuda'))\n",
    "model = model.to(device)\n",
    "\n",
    "model.configure_optimizers(lr=learning_rate)\n",
    "loss_fun = CombinedLoss()\n",
    "model.change_loss_function(loss_fun)\n",
    "\n",
    "batch_size = batch_size\n",
    "\n",
    "traindataset = AudioCleaningDataset(directoryBase+locationTrainFile,\n",
    "                                    directoryBase+'/extra/audiosDivididos',\n",
    "                                    directoryBase+'/extra/ruidosDivididos',\n",
    "                                    directoryBase+\"/wavenet/CSV/ruido_train.csv\",\n",
    "                                    directoryBase+\"/extra/irDivididos/irtrain\", \n",
    "                                    maxRuido=maxRuido, fixedInterval=False)\n",
    "\n",
    "\n",
    "valdataset = AudioCleaningDataset(directoryBase+locationValidationFile, \n",
    "                                  directoryBase+'/extra/audiosDivididos', \n",
    "                                 directoryBase+'/extra/ruidosDivididos',\n",
    "                                  directoryBase+\"/wavenet/CSV/ruido_validation.csv\",\n",
    "                                  directoryBase+\"/extra/irDivididos/irval\",\n",
    "                                  maxRuido=maxRuido, fixedInterval=True)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=min_delta, patience=patience, verbose=True, mode=\"min\")\n",
    "\n",
    "\n",
    "noiseCallback1 = DynamicNoiseCallback(traindataset, valdataset, params[\"maxRuido\"]*1.2, 5000, 1000)\n",
    "noiseCallback2 = DynamicNoiseCallback(traindataset, valdataset, params[\"maxRuido\"]*1.4, 1000, 900)\n",
    "noiseCallback3 = DynamicNoiseCallback(traindataset, valdataset, params[\"maxRuido\"]*1.6, 900, 500)\n",
    "noiseCallback4 = DynamicNoiseCallback(traindataset, valdataset, params[\"maxRuido\"]*1.8, 500, 300)\n",
    "noiseCallback5 = DynamicNoiseCallback(traindataset, valdataset, params[\"maxRuido\"]*2, 300, 100)\n",
    "\n",
    "snrCallback1 = DynamicNoiseCallback(traindataset, valdataset, params[\"snr\"]*1.2, 5000, 1000)\n",
    "snrCallback2 = DynamicNoiseCallback(traindataset, valdataset, params[\"snr\"]*1.4, 1000, 900)\n",
    "snrCallback3 = DynamicNoiseCallback(traindataset, valdataset, params[\"snr\"]*1.6, 900, 500)\n",
    "snrCallback4 = DynamicNoiseCallback(traindataset, valdataset, params[\"snr\"]*1.8, 500, 300)\n",
    "snrCallback5 = DynamicNoiseCallback(traindataset, valdataset, params[\"snr\"]*2, 300, 100)\n",
    "\n",
    "irCall = IrCallback(traindataset, valdataset, 450, 0)\n",
    "\n",
    "calls = [early_stop_callback, noiseCallback1,noiseCallback2,noiseCallback3,noiseCallback4,noiseCallback5,\n",
    "        snrCallback1,snrCallback2,snrCallback3,snrCallback4,snrCallback5, irCall]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(calls)):\n",
    "    print(calls[i].trainDataset == calls[i].valDataset)\n",
    "    #print(calls[i].valDataset == valdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset.ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                     | Params\n",
      "-----------------------------------------------------------------------\n",
      "0 | causal    | CausalConv1d                             | 256   \n",
      "1 | res_stack | ResidualStack                            | 0     \n",
      "2 | densnet   | DensNet                                  | 8     \n",
      "3 | loss_fun  | CombinedLoss                             | 0     \n",
      "4 | srmr      | SpeechReverberationModulationEnergyRatio | 0     \n",
      "5 | wb_pesq   | PerceptualEvaluationSpeechQuality        | 0     \n",
      "6 | stoi      | ShortTimeObjectiveIntelligibility        | 0     \n",
      "7 | snr       | SignalNoiseRatio                         | 0     \n",
      "-----------------------------------------------------------------------\n",
      "264       Trainable params\n",
      "0         Non-trainable params\n",
      "264       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07326581c99c453daf4bf141a2f03fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages/pystoi/stoi.py:66: RuntimeWarning: Not enough STFT frames to compute intermediate intelligibility measure after removing silent frames. Returning 1e-5. Please check you wav files\n",
      "  warnings.warn('Not enough STFT frames to compute intermediate '\n",
      "/home/afridman/miniconda3/envs/myenv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1,\n",
    "                     max_epochs=max_epochs,\n",
    "                     log_every_n_steps=log_every_n_steps,\n",
    "                     callbacks=calls,\n",
    "                    accumulate_grad_batches=accumulate_grad_batches\n",
    "                     ,val_check_interval=val_check_interval  # Perform validation every 10 training steps\n",
    "                     ,reload_dataloaders_every_n_epochs=2\n",
    ")\n",
    "\n",
    "traindataloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "valdataloader = DataLoader(valdataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=traindataloader\n",
    "            , val_dataloaders=valdataloader\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.val_dataloaders.dataset.ir"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
